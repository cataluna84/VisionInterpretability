{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be898514",
   "metadata": {
    "id": "colab-badge"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/cataluna84/VisionInterpretability/blob/main/notebooks/cataluna84__segment_3_dataset_images.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49240255",
   "metadata": {
    "id": "colab-setup"
   },
   "outputs": [],
   "source": [
    "# @title ðŸš€ Environment Setup (Run this cell first!)\n",
    "# @markdown This cell sets up the environment for both Colab and local runs.\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Check if running in Google Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"ðŸŒ Running in Google Colab\")\n",
    "    \n",
    "    # Clone the repository if not already cloned\n",
    "    if not os.path.exists('VisionInterpretability'):\n",
    "        !git clone https://github.com/cataluna84/VisionInterpretability.git\n",
    "    \n",
    "    # Change to project directory\n",
    "    os.chdir('VisionInterpretability')\n",
    "    \n",
    "    # Install dependencies\n",
    "    !pip install -q torch torchvision matplotlib numpy pillow tqdm opencv-python requests torch-lucent\n",
    "    \n",
    "    # Add src to Python path\n",
    "    sys.path.insert(0, 'src')\n",
    "    print(\"âœ… Colab setup complete!\")\n",
    "else:\n",
    "    print(\"ðŸ’» Running locally\")\n",
    "    # For local runs, add src to path if running from notebooks directory\n",
    "    if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "        sys.path.insert(0, os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
    "    elif 'src' not in sys.path:\n",
    "        sys.path.insert(0, 'src')\n",
    "    print(\"âœ… Local setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36874ae6",
   "metadata": {},
   "source": [
    "# Detailed Analysis: Feature Visualization & Dataset Examples\n",
    "## InceptionV1 - Layer mixed4a\n",
    "\n",
    "This notebook provides a rigorous exploration of **INTERPRETABILITY** in Convolutional Neural Networks (CNNs). we focus on the `mixed4a` layer of InceptionV1 (GoogLeNet).\n",
    "\n",
    "### The Goal\n",
    "We aim to answer: **\"What features is the network looking for?\"**\n",
    "We use two complementary approaches:\n",
    "1.  **Dataset Examples**: Finding real images from the dataset (ImageNet) that maximally activate a specific neuron.\n",
    "    *   *Question:* \"What real-world patterns trigger this neuron?\"\n",
    "2.  **Feature Visualization (Optimization)**: Synthesizing an \"ideal\" input image that maximizes the neuron's activation using gradient ascent.\n",
    "    *   *Question:* \"What is the 'platonic ideal' of the feature this neuron detects?\"\n",
    "\n",
    "---\n",
    "### Theoretical Foundations\n",
    "\n",
    "#### 1. Forward Pass & Activation\n",
    "A neural network is a function $f: X \\to Y$.\n",
    "For an input image $x$, the activation $A_{n,l}(x)$ represents the output of the $n$-th neuron at layer $l$.\n",
    "We refer to this map $A_{n,l}(x)$ as the \"Feature Map\".\n",
    "\n",
    "#### 2. Optimization (Feature Visualization)\n",
    "To visualize what a neuron wants, we perform **Optimization on the Input**.\n",
    "Unlike training (where we update weights $w$ to minimize loss), here we freeze the weights and update the image $x$.\n",
    "\n",
    "**The Objective:**\n",
    "$$ x^* = \\arg\\max_x (A_n(x) - \\lambda R(x)) $$\n",
    "\n",
    "Where:\n",
    "*   $A_n(x)$: The activation of the target neuron.\n",
    "*   $R(x)$: Regularization terms (e.g., total variation, frequency penalties) to ensure the image looks \"natural\" and not just high-frequency adversarial noise.\n",
    "\n",
    "**The Update Rule (Gradient Ascent):**\n",
    "$$ x_{t+1} = x_t + \\eta \\nabla_x (A_n(x_t) - \\lambda R(x_t)) $$\n",
    "\n",
    "We use the **Lucent** library (PyTorch port of Lucid) which implements robust optimization techniques:\n",
    "*   **Fourier Parameterization**: Optimizing in frequency space (decorrelated) speeds up convergence.\n",
    "*   **Transformation Robustness**: We maximize the expected activation under random jitters/scales: $E_{t \\sim T}[A_n(t(x))]$. This forces the feature to be robust and identifiable.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b26d528",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration\n",
    "\n",
    "We initialize the environment, load the pre-trained InceptionV1 model, and configure our experiment parameters.\n",
    "We use `wandb` to log our experiments for reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path for local imports\n",
    "project_root = Path.cwd().parent\n",
    "src_path = project_root / \"src\"\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(project_root / \".env\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    \"num_neurons\": 10,           # First 10 neurons of mixed4a (512 total)\n",
    "    \"samples_per_category\": 9,    # 9 samples per category (min, slight-, slight+, max)\n",
    "    \"max_samples\": None,         # Process ALL samples (WARNING: ~1.28M images)\n",
    "                                 # Set to 1000 or 10000 for faster testing.\n",
    "    \"batch_size\": 1024,             # Batch size for streaming inference\n",
    "    \"layer_name\": \"mixed4a\",      # Target layer in InceptionV1\n",
    "    \"wandb_project\": \"vision-interpretability\",\n",
    "    \"wandb_run_name\": \"mixed4a-activation-spectrum-v3\",\n",
    "    \"generate_optimized\": True,   # Generate gradient-ascent visualizations\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wandb-init",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "# Initialize WANDB run\n",
    "run = wandb.init(\n",
    "    project=CONFIG[\"wandb_project\"],\n",
    "    name=CONFIG[\"wandb_run_name\"],\n",
    "    config=CONFIG,\n",
    ")\n",
    "\n",
    "print(f\"WANDB Run: {run.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32ed45f",
   "metadata": {},
   "source": [
    "## 2. Theory: Activation Extraction\n",
    "\n",
    "We load the model and attach hooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lucent.modelzoo import inceptionv1\n",
    "\n",
    "# Load pretrained InceptionV1\n",
    "model = inceptionv1(pretrained=True)\n",
    "model.to(device).eval()\n",
    "\n",
    "print(\"InceptionV1 loaded successfully!\")\n",
    "print(f\"\\nModel layers containing 'mixed4a':\")\n",
    "for name, module in model.named_modules():\n",
    "    if \"mixed4a\" in name:\n",
    "        print(f\"  {name}: {type(module).__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-extractor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_3_dataset_images.activation_pipeline import (\n",
    "    ActivationExtractor,\n",
    "    ActivationSpectrumTrackerV2,\n",
    "    SampleRecord,\n",
    ")\n",
    "\n",
    "# Register forward hook on mixed4a\n",
    "extractor = ActivationExtractor(model, CONFIG[\"layer_name\"])\n",
    "\n",
    "# Initialize spectrum tracker\n",
    "tracker = ActivationSpectrumTrackerV2(\n",
    "    num_neurons=CONFIG[\"num_neurons\"],\n",
    "    samples_per_category=CONFIG[\"samples_per_category\"],\n",
    ")\n",
    "\n",
    "print(f\"Tracking {CONFIG['num_neurons']} neurons with {CONFIG['samples_per_category']} samples per category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94439c2",
   "metadata": {},
   "source": [
    "## 2. Theory: Dataset Processing (Max-Min Search)\n",
    "\n",
    "To find **Dataset Examples**, we scan the entire ImageNet dataset (1.2M images).\n",
    "For each image $x_i$, we compute the activation map $A(x_i)$.\n",
    "Since $A(x_i)$ is spatial ($H \\times W$), we take the maximum value across spatial dimensions:\n",
    "$$ a_i = \\max_{h,w} A(x_i)_{h,w} $$\n",
    "\n",
    "We track four categories of samples for each neuron:\n",
    "1.  **Maximum ($a_i \\gg 0$)**: The images that excite the neuron the most.\n",
    "2.  **Slightly Positive ($a_i > 0$)**: Weak activation.\n",
    "3.  **Slightly Negative ($a_i < 0$)**: Weak inhibition.\n",
    "4.  **Minimum ($a_i \\ll 0$)**: The images that maximally suppress the neuron (if the activation function allows negative values, e.g., before ReLU, or if considering the pre-activation).\n",
    "\n",
    "**Streaming vs. Downloading**:\n",
    "The dataset is huge (~150GB). We use **HuggingFace Streaming** to process images on-the-fly without downloading the full dataset to disk. Ideally, this allows inspection of massive datasets with limited storage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_3_dataset_images.activation_pipeline import ImageNetStreamer\n",
    "\n",
    "# Setup streaming from HuggingFace\n",
    "streamer = ImageNetStreamer(\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    "    max_samples=CONFIG[\"max_samples\"],\n",
    ")\n",
    "\n",
    "max_samples_str = f\"{CONFIG['max_samples']:,}\" if CONFIG['max_samples'] is not None else \"ALL\"\n",
    "print(f\"Streaming up to {max_samples_str} samples from ImageNet-1k\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
    "\n",
    "if CONFIG['max_samples'] is not None:\n",
    "    print(f\"Estimated batches: {CONFIG['max_samples'] // CONFIG['batch_size']}\")\n",
    "else:\n",
    "    print(\"Estimated batches: Unknown (streaming infinite/all)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a95d60f",
   "metadata": {},
   "source": [
    "## 3. Batch Processing Loop\n",
    "\n",
    "This is the core execution loop.\n",
    "*   **Input**: Stream of ImageNet batches.\n",
    "*   **Process**:\n",
    "    1.  **Inference**: Forward pass through InceptionV1.\n",
    "    2.  **Extraction**: `ActivationExtractor` hooks grab the tensor at `mixed4a`.\n",
    "    3.  **Tracking**: `ActivationSpectrumTracker` updates the \"Top-K\" and \"Bottom-K\" lists using a min-heap algorithm for efficiency.\n",
    "*   **Logging**:\n",
    "    We log **throughput** (images/sec) to WandB to monitor performance. Since streaming can be network-bound, this metric helps diagnose bottlenecks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batch-processing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process batches with TIMING and LOGGING\n",
    "import time\n",
    "\n",
    "total_processed = 0\n",
    "batch_count = 0\n",
    "\n",
    "print(f\"Processing batches using device: {device}...\")\n",
    "t_start_total = time.time()\n",
    "\n",
    "for tensors, images, labels, ids in tqdm(streamer, desc=\"Batches\"):\n",
    "    t_batch_start = time.time()\n",
    "    \n",
    "    # 1. Move Data to GPU (Bottleneck: Host-to-Device transfer)\n",
    "    tensors = tensors.to(device)\n",
    "    \n",
    "    # 2. Forward Pass (Inference)\n",
    "    # hooks in ActivationExtractor capture the data automatically\n",
    "    with torch.no_grad():\n",
    "        _ = model(tensors)\n",
    "    \n",
    "    # 3. Extract Activations\n",
    "    # get_max_activations_per_channel performs Global Max Pooling (B, C, H, W) -> (B, C)\n",
    "    activations = extractor.get_max_activations_per_channel()\n",
    "    \n",
    "    # 4. Update Tracker (CPU)\n",
    "    # We move activations back to CPU for sorting/tracking to avoid GPU sync stalls\n",
    "    tracker.update(\n",
    "        activations[:, :CONFIG[\"num_neurons\"]],\n",
    "        images,\n",
    "        ids,\n",
    "        labels,\n",
    "    )\n",
    "    \n",
    "    # Timing calculations\n",
    "    batch_duration = time.time() - t_batch_start\n",
    "    current_batch_size = len(tensors)\n",
    "    throughput = current_batch_size / batch_duration\n",
    "    \n",
    "    total_processed += current_batch_size\n",
    "    batch_count += 1\n",
    "    \n",
    "    # 5. Logging to WandB\n",
    "    # We allow the user to monitor performance in real-time\n",
    "    if batch_count % 10 == 0:\n",
    "        wandb.log({\n",
    "            \"batch\": batch_count,\n",
    "            \"samples_processed\": total_processed,\n",
    "            \"batch_duration_sec\": batch_duration,\n",
    "            \"throughput_img_per_sec\": throughput,\n",
    "        })\n",
    "\n",
    "t_end_total = time.time()\n",
    "print(f\"\\nProcessed {total_processed:,} samples in {batch_count} batches\")\n",
    "print(f\"Total time: {t_end_total - t_start_total:.2f} seconds\")\n",
    "print(f\"Average throughput: {total_processed / (t_end_total - t_start_total):.2f} img/sec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fface6f",
   "metadata": {},
   "source": [
    "## 4. Generating \"Dream\" Images (Optimization)\n",
    "\n",
    "Here we generate the **Synthetic Examples**.\n",
    "We use `torch-lucent` to perform the gradient ascent described in the Theory section.\n",
    "\n",
    "**Why Generate?**\n",
    "Dataset examples can only show us *correlation*. If a neuron activates for a \"dog head\", is it detecting the *eyes*, the *snout*, or the *fur texture*?\n",
    "Optimization isolates the causal feature. If the optimized image contains *only* a snout pattern, we know the neuron is a \"snout detector\".\n",
    "\n",
    "**Negative Optimization**:\n",
    "We also optimize for the *negative* objective:\n",
    "$$ x^*_{neg} = \\arg\\min_x A_n(x) = \\arg\\max_x (-A_n(x)) $$\n",
    "This shows us what pattern the neuron *actively avoids* or is inhibited by.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-optimized",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lucent.optvis import render, objectives\n",
    "\n",
    "optimized_images = {}  # Positive optimization (maximize activation)\n",
    "optimized_negative_images = {}  # Negative optimization (minimize activation)\n",
    "\n",
    "if CONFIG[\"generate_optimized\"]:\n",
    "    print(f\"Generating optimized examples for {CONFIG['num_neurons']} neurons...\")\n",
    "    print(\"This generates TWO images per neuron:\")\n",
    "    print(\"  - Positive: What the neuron WANTS to see (maximize activation)\")\n",
    "    print(\"  - Negative: What the neuron AVOIDS (minimize activation)\")\n",
    "    print()\n",
    "    \n",
    "    for n in tqdm(range(CONFIG[\"num_neurons\"]), desc=\"Optimizing\"):\n",
    "        # --- Positive Optimization (maximize activation) ---\n",
    "        try:\n",
    "            objective = f\"{CONFIG['layer_name']}:{n}\"\n",
    "            result = render.render_vis(\n",
    "                model,\n",
    "                objective,\n",
    "                show_image=False,\n",
    "                show_inline=False,\n",
    "                thresholds=(512,),\n",
    "            )\n",
    "            \n",
    "            if result and len(result) > 0:\n",
    "                img_array = result[0][0]\n",
    "                img_array = (img_array * 255).astype(np.uint8)\n",
    "                optimized_images[n] = Image.fromarray(img_array)\n",
    "            else:\n",
    "                optimized_images[n] = None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Failed positive optimization for neuron {n}: {e}\")\n",
    "            optimized_images[n] = None\n",
    "        \n",
    "        # --- Negative Optimization (minimize activation) ---\n",
    "        try:\n",
    "            # Create a negated channel objective using lucent's objectives API\n",
    "            positive_objective = objectives.channel(CONFIG['layer_name'], n)\n",
    "            negative_objective = -1 * positive_objective\n",
    "            \n",
    "            result = render.render_vis(\n",
    "                model,\n",
    "                negative_objective,\n",
    "                show_image=False,\n",
    "                show_inline=False,\n",
    "                thresholds=(512,),\n",
    "            )\n",
    "            \n",
    "            if result and len(result) > 0:\n",
    "                img_array = result[0][0]\n",
    "                img_array = (img_array * 255).astype(np.uint8)\n",
    "                optimized_negative_images[n] = Image.fromarray(img_array)\n",
    "            else:\n",
    "                optimized_negative_images[n] = None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Failed negative optimization for neuron {n}: {e}\")\n",
    "            optimized_negative_images[n] = None\n",
    "    \n",
    "    pos_count = sum(1 for v in optimized_images.values() if v is not None)\n",
    "    neg_count = sum(1 for v in optimized_negative_images.values() if v is not None)\n",
    "    print(f\"Generated {pos_count} positive and {neg_count} negative optimized images\")\n",
    "else:\n",
    "    print(\"Skipping optimized example generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cdaa25",
   "metadata": {},
   "source": [
    "## 5. Visualization: The Distill Spectrum\n",
    "\n",
    "We visualize the results using the layout pioneered by Distill.pub.\n",
    "The **Activation Spectrum** visualization arranges samples from \"Most Negative\" to \"Most Positive\".\n",
    "\n",
    "**Layout:**\n",
    "| Neg Optimized | Min Dataset | Slight Neg | Slight Pos | Max Dataset | Pos Optimized |\n",
    "| :---: | :---: | :---: | :---: | :---: | :---: |\n",
    "| *What it hates* | *Real suppressant* | *Weak suppressant* | *Weak trigger* | *Real trigger* | *What it loves* |\n",
    "\n",
    "This provides a holistic view of the neuron's behavior, verifying if the synthetic \"dream\" matches the real-world triggers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-spectrum",
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_3_dataset_images.visualization import plot_neuron_spectrum_distill\n",
    "\n",
    "# The plot_neuron_spectrum_distill function creates Distill.pub style visualizations\n",
    "# showing the full activation spectrum:\n",
    "# | Neg Optimized | Min Grid | Slight- | Slight+ | Max Grid | Pos Optimized |\n",
    "print(f\"Using Distill.pub style visualization for {CONFIG['num_neurons']} neurons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-all-neurons",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize spectrum for all tracked neurons using Distill.pub style\n",
    "print(f\"Visualizing activation spectrum for {CONFIG['num_neurons']} neurons...\\n\")\n",
    "\n",
    "figures = []\n",
    "for n in range(CONFIG[\"num_neurons\"]):\n",
    "    spectrum = tracker.get_spectrum(n)\n",
    "    pos_img = optimized_images.get(n)\n",
    "    neg_img = optimized_negative_images.get(n)\n",
    "    \n",
    "    # Use Distill.pub style visualization from the visualization module\n",
    "    fig = plot_neuron_spectrum_distill(\n",
    "        neuron_idx=n,\n",
    "        layer_name=CONFIG[\"layer_name\"],\n",
    "        spectrum=spectrum,\n",
    "        optimized_img=pos_img,\n",
    "        negative_optimized_img=neg_img,\n",
    "    )\n",
    "    figures.append(fig)\n",
    "    plt.show()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving-markdown",
   "metadata": {},
   "source": [
    "## 6. Saving Results\n",
    "\n",
    "We save the generated visualizations to the local file system for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save-figures",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create results directory\n",
    "results_dir = project_root / \"notebooks\" / \"results\" / \"segment_3_dataset_images\"\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Saving results to: {results_dir}\")\n",
    "\n",
    "# Initialize WandB Table for logging chart\n",
    "wandb_table = wandb.Table(columns=[\"Neuron Index\", \"Activation Spectrum\"])\n",
    "\n",
    "if 'figures' in locals() and figures:\n",
    "    for n, fig in enumerate(figures):\n",
    "        # Save locally\n",
    "        save_path = results_dir / f\"neuron_{n}_spectrum.png\"\n",
    "        fig.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"  Saved: {save_path.name}\")\n",
    "        \n",
    "        # Add to WandB Table\n",
    "        # converting figure to image for logging\n",
    "        try:\n",
    "            wandb_table.add_data(n, wandb.Image(fig))\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: Could not add figure {n} to WandB table: {e}\")\n",
    "else:\n",
    "    print(\"No figures found to save.\")\n",
    "\n",
    "# Log the table to WandB\n",
    "if wandb.run is not None:\n",
    "    wandb.log({\"activation_spectrum_chart\": wandb_table})\n",
    "    print(\"Logged activation spectrum table to WandB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "In this notebook, we explored the `mixed4a` layer of InceptionV1 using two complementary techniques:\n",
    "1.  **Dataset Examples**: We found real images that maximally activate specific neurons. This tells us *what* the neuron responds to in the real world.\n",
    "2.  **Feature Visualization**: We synthesized 'dream' images via gradient ascent. This tells us the *purest form* of the feature the neuron detects.\n",
    "\n",
    "**Key Takeaways:**\n",
    "*   By comparing the 'Dataset Examples' with the 'Optimized' images, we can verify if our interpretation of the neuron is correct.\n",
    "*   If the dataset examples match the optimized feature (e.g., both look like 'dog heads'), we discovered a robust feature.\n",
    "*   If they differ, the neuron might be polysemantic (detecting multiple unrelated concepts) or responding to context/background correlations.\n",
    "\n",
    "This 'Spectrum' visualization (Dataset Min/Max + Optimization) is a powerful tool for rigorous interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleanup-markdown",
   "metadata": {},
   "source": [
    "## 8. Cleanup\n",
    "\n",
    "We finish the WandB run to ensure all logs are synced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee101c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish WandB run\n",
    "if wandb.run is not None:\n",
    "    wandb.finish()\n",
    "    print(\"WandB run finished.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
