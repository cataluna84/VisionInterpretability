{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b497c48",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cataluna84/VisionInterpretability/blob/main/notebooks/cataluna84__segment_1_intro.ipynb)\n",
    "\n",
    "# Segment 1: CNN Basics, Decoding CNNs & Interpretability\n",
    "\n",
    "Welcome to this interactive tutorial on **Computer Vision** and **Convolutional Neural Networks (CNNs)**.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Image Representation** \u2014 How computers \"see\" images as tensors\n",
    "2. **Convolution Operations** \u2014 The math behind edge detection, blur, and sharpening\n",
    "3. **Building a CNN** \u2014 Train a model from scratch on ImageNette\n",
    "4. **Feature Visualization** \u2014 See what patterns each layer detects\n",
    "5. **Interpretability Methods** \u2014 Understand *why* a model makes predictions\n",
    "   - Saliency Maps (Vanilla Gradients)\n",
    "   - Grad-CAM (Class Activation Mapping)\n",
    "\n",
    "Let's decode the black box! \ud83e\udde0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colab_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title \ud83d\ude80 Environment Setup (Run this cell first!)\n",
    "# @markdown This cell sets up the environment for both Colab and local runs.\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Check if running in Google Colab\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"\ud83c\udf10 Running in Google Colab\")\n",
    "    \n",
    "    # Clone the repository if not already cloned\n",
    "    if not os.path.exists('VisionInterpretability'):\n",
    "        !git clone https://github.com/cataluna84/VisionInterpretability.git\n",
    "    \n",
    "    # Change to project directory\n",
    "    os.chdir('VisionInterpretability')\n",
    "    \n",
    "    # Install dependencies\n",
    "    !pip install -q torch torchvision matplotlib numpy pillow tqdm opencv-python requests\n",
    "    \n",
    "    # Add src to Python path\n",
    "    sys.path.insert(0, 'src')\n",
    "    print(\"\u2705 Colab setup complete!\")\n",
    "else:\n",
    "    print(\"\ud83d\udcbb Running locally\")\n",
    "    # For local runs, add src to path if running from notebooks directory\n",
    "    if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "        sys.path.insert(0, os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
    "    elif 'src' not in sys.path:\n",
    "        sys.path.insert(0, 'src')\n",
    "    print(\"\u2705 Local setup complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288b3f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Import required libraries and custom modules.\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Import our custom modules\n",
    "from segment_1_intro import models, visualize, data\n",
    "\n",
    "# Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"\ud83d\udda5\ufe0f Using device: {device}\")\n",
    "\n",
    "# Visualization settings\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66770ebc",
   "metadata": {},
   "source": [
    "## 1. How Images Are Represented\n",
    "\n",
    "### Theoretical Background\n",
    "\n",
    "Digital images are represented as **multi-dimensional arrays** (tensors) where each element corresponds to a pixel intensity. In deep learning frameworks like PyTorch, images follow the **CHW convention**:\n",
    "\n",
    "$$\\mathbf{X} \\in \\mathbb{R}^{C \\times H \\times W}$$\n",
    "\n",
    "where:\n",
    "- $C$ = Number of color channels (3 for RGB: Red, Green, Blue)\n",
    "- $H$ = Height in pixels\n",
    "- $W$ = Width in pixels\n",
    "\n",
    "### Pixel Value Representation\n",
    "\n",
    "| Format | Range | Description |\n",
    "|--------|-------|-------------|\n",
    "| Raw (uint8) | [0, 255] | Original image format |\n",
    "| Normalized | [0, 1] | After `ToTensor()` transform |\n",
    "| Standardized | ~[-2.5, 2.5] | After ImageNet normalization |\n",
    "\n",
    "**ImageNet normalization** is standard for pretrained models:\n",
    "\n",
    "$$x_{\\text{norm}} = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "where $\\mu = [0.485, 0.456, 0.406]$ and $\\sigma = [0.229, 0.224, 0.225]$ (per channel).\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "Understanding tensor representations is fundamental because:\n",
    "1. **Convolutions** operate on spatial dimensions (H, W)\n",
    "2. **Batch processing** adds a 4th dimension: $(N, C, H, W)$\n",
    "3. **Interpretability methods** compute gradients with respect to these pixel values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ab4b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a sample image\n",
    "url = \"https://images.unsplash.com/photo-1543466835-00a7907e9de1?ixlib=rb-4.0.3&q=85&fm=jpg&crop=entropy&cs=srgb&w=512\"\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "\n",
    "# Display original image\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "axes[0].imshow(img)\n",
    "axes[0].set_title(\"Original Image\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Convert to tensor and show individual channels\n",
    "img_tensor = transforms.ToTensor()(img)\n",
    "print(f\"Tensor Shape: {img_tensor.shape} (C, H, W)\")\n",
    "print(f\"Value Range: [{img_tensor.min():.3f}, {img_tensor.max():.3f}]\")\n",
    "\n",
    "# Show RGB channels\n",
    "channel_names = [\"Red Channel\", \"Green Channel\", \"Blue Channel\"]\n",
    "cmaps = [\"Reds\", \"Greens\", \"Blues\"]\n",
    "\n",
    "for i, (name, cmap) in enumerate(zip(channel_names, cmaps)):\n",
    "    axes[i+1].imshow(img_tensor[i], cmap=cmap)\n",
    "    axes[i+1].set_title(name)\n",
    "    axes[i+1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad78e886",
   "metadata": {},
   "source": [
    "## 2. Convolution Operations Deep Dive\n",
    "\n",
    "### Mathematical Definition\n",
    "\n",
    "A **2D convolution** (technically cross-correlation in deep learning) applies a kernel $\\mathbf{K}$ to an input image $\\mathbf{I}$ to produce an output feature map $\\mathbf{S}$:\n",
    "\n",
    "$$S(i, j) = (\\mathbf{I} * \\mathbf{K})(i, j) = \\sum_{m=0}^{M-1} \\sum_{n=0}^{N-1} I(i+m, j+n) \\cdot K(m, n)$$\n",
    "\n",
    "where $M \\times N$ is the kernel size.\n",
    "\n",
    "### Output Size Calculation\n",
    "\n",
    "Given input size $i$, kernel size $k$, stride $s$, and padding $p$:\n",
    "\n",
    "$$o = \\left\\lfloor \\frac{i + 2p - k}{s} \\right\\rfloor + 1$$\n",
    "\n",
    "### Common Kernels and Their Effects\n",
    "\n",
    "| Kernel | Purpose | Mathematical Property |\n",
    "|--------|---------|----------------------|\n",
    "| **Sobel** | Edge detection | Approximates gradient $\\nabla I$ |\n",
    "| **Laplacian** | All edges | Second derivative $\\nabla^2 I$ |\n",
    "| **Gaussian** | Blur/smoothing | Low-pass filter |\n",
    "| **Sharpen** | Enhance edges | High-pass amplification |\n",
    "\n",
    "### Sobel Operators\n",
    "\n",
    "Detect horizontal and vertical edges:\n",
    "\n",
    "$$G_x = \\begin{bmatrix} -1 & 0 & +1 \\\\ -2 & 0 & +2 \\\\ -1 & 0 & +1 \\end{bmatrix}, \\quad G_y = \\begin{bmatrix} -1 & -2 & -1 \\\\ 0 & 0 & 0 \\\\ +1 & +2 & +1 \\end{bmatrix}$$\n",
    "\n",
    "Edge magnitude: $G = \\sqrt{G_x^2 + G_y^2}$\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "> In CNNs, kernels are **learned** through backpropagation rather than hand-designed, allowing the network to discover optimal feature detectors for the task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01d9487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_convolution(image_tensor, kernel):\n",
    "    \"\"\"\n",
    "    Simple manual 2D convolution for demonstration.\n",
    "    image_tensor: (1, H, W) -> Grayscale\n",
    "    kernel: (K, K) -> Filter\n",
    "    \"\"\"\n",
    "    c, h, w = image_tensor.shape\n",
    "    kh, kw = kernel.shape\n",
    "    output_h, output_w = h - kh + 1, w - kw + 1\n",
    "    output = torch.zeros((output_h, output_w))\n",
    "    \n",
    "    for i in range(output_h):\n",
    "        for j in range(output_w):\n",
    "            region = image_tensor[0, i:i+kh, j:j+kw]\n",
    "            output[i, j] = torch.sum(region * kernel)\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Define various filters\n",
    "kernels = {\n",
    "    \"Sobel X (Vertical Edges)\": torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32),\n",
    "    \"Sobel Y (Horizontal Edges)\": torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32),\n",
    "    \"Laplacian (All Edges)\": torch.tensor([[0, 1, 0], [1, -4, 1], [0, 1, 0]], dtype=torch.float32),\n",
    "    \"Gaussian Blur\": torch.tensor([[1, 2, 1], [2, 4, 2], [1, 2, 1]], dtype=torch.float32) / 16,\n",
    "    \"Sharpen\": torch.tensor([[0, -1, 0], [-1, 5, -1], [0, -1, 0]], dtype=torch.float32),\n",
    "}\n",
    "\n",
    "# Convert to grayscale\n",
    "gray_img = transforms.Grayscale()(img_tensor)\n",
    "\n",
    "# Apply each filter\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "axes[0].imshow(gray_img.squeeze(), cmap='gray')\n",
    "axes[0].set_title(\"Original (Grayscale)\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "for idx, (name, kernel) in enumerate(kernels.items(), 1):\n",
    "    result = manual_convolution(gray_img, kernel)\n",
    "    axes[idx].imshow(result.abs(), cmap='gray')\n",
    "    axes[idx].set_title(name)\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle(\"Convolution Filter Effects\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dce2c60",
   "metadata": {},
   "source": [
    "## 3. Building & Training a CNN on ImageNette\n",
    "\n",
    "### CNN Architecture Theory\n",
    "\n",
    "A Convolutional Neural Network learns a hierarchy of features through stacked layers:\n",
    "\n",
    "$$\\mathbf{h}^{(l)} = \\sigma\\left(\\mathbf{W}^{(l)} * \\mathbf{h}^{(l-1)} + \\mathbf{b}^{(l)}\\right)$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{h}^{(l)}$ = Activation at layer $l$\n",
    "- $\\mathbf{W}^{(l)}$ = Learnable convolution kernels\n",
    "- $\\sigma$ = Non-linear activation (e.g., ReLU)\n",
    "- $*$ = Convolution operation\n",
    "\n",
    "### The Feature Hierarchy\n",
    "\n",
    "Research by Zeiler & Fergus (2014) and Olah et al. (2017) shows:\n",
    "\n",
    "| Layer Depth | Features Learned | Examples |\n",
    "|-------------|-----------------|----------|\n",
    "| Layer 1 | Low-level | Edges, colors, gradients |\n",
    "| Layer 2-3 | Mid-level | Textures, patterns, curves |\n",
    "| Layer 4+ | High-level | Object parts, faces, wheels |\n",
    "| Final | Semantic | Full objects, scenes |\n",
    "\n",
    "### ImageNette Dataset\n",
    "\n",
    "A subset of ImageNet with 10 easily classifiable classes:\n",
    "\n",
    "| Class | ImageNet ID | Category |\n",
    "|-------|------------|----------|\n",
    "| Tench | n01440764 | Fish |\n",
    "| English Springer | n02102040 | Dog |\n",
    "| Cassette Player | n02979186 | Electronics |\n",
    "| Chain Saw | n03000684 | Tool |\n",
    "| Church | n03028079 | Building |\n",
    "| French Horn | n03394916 | Instrument |\n",
    "| Garbage Truck | n03417042 | Vehicle |\n",
    "| Gas Pump | n03425413 | Object |\n",
    "| Golf Ball | n03445777 | Sports |\n",
    "| Parachute | n03888257 | Equipment |\n",
    "\n",
    "### Training Objective\n",
    "\n",
    "We minimize the **Cross-Entropy Loss**:\n",
    "\n",
    "$$\\mathcal{L} = -\\sum_{c=1}^{C} y_c \\log(\\hat{y}_c)$$\n",
    "\n",
    "where $y_c$ is the one-hot target and $\\hat{y}_c$ is the predicted probability for class $c$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ea20f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ImageNette dataset from fast.ai S3\n",
    "print(\"\ud83d\udce6 Loading ImageNette dataset from fast.ai S3...\")\n",
    "train_loader = data.load_imagenette(split=\"train\", image_size=128, batch_size=32),\n",
    "    data_dir=\"../data\"\n",
    "val_loader = data.load_imagenette(split=\"validation\", image_size=128, batch_size=32),\n",
    "    data_dir=\"../data\",\n",
    "\n",
    "print(f\"\u2705 Training samples: ~{len(train_loader) * 32}\")\n",
    "print(f\"\u2705 Validation samples: ~{len(val_loader) * 32}\")\n",
    "print(f\"\ud83d\udccb Classes: {data.IMAGENETTE_CLASSES}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab14bd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some training samples\n",
    "sample_images, sample_labels = data.get_sample_images(train_loader, num_samples=8)\n",
    "classes = data.IMAGENETTE_CLASSES\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(14, 7))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    img = visualize.denormalize_image(sample_images[i])\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(classes[sample_labels[i]])\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle(\"ImageNette Training Samples\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0350aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our SimpleCNN model\n",
    "model = models.load_simple_cnn(num_classes=10)\n",
    "print(model)\n",
    "print(f\"\\n\ud83d\udcca Total Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8d6313",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "Training uses the **Adam optimizer**, which adapts learning rates per-parameter:\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t$$\n",
    "\n",
    "where $\\hat{m}_t$ and $\\hat{v}_t$ are bias-corrected first and second moment estimates.\n",
    "\n",
    "\u26a0\ufe0f **Note**: Training takes a few minutes. For a quick demo, we train for just 3 epochs.\n",
    "In practice, you'd train for 20+ epochs to achieve better accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db77426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model (reduced epochs for demo)\n",
    "EPOCHS = 3  # Increase to 10-20 for better results\n",
    "\n",
    "print(f\"\ud83d\ude80 Training SimpleCNN for {EPOCHS} epochs...\")\n",
    "history = models.train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=EPOCHS,\n",
    "    learning_rate=0.001,\n",
    "    device=device,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "print(\"\\n\u2705 Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715a8b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "visualize.plot_training_history(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af79e99",
   "metadata": {},
   "source": [
    "## 4. Feature Map Visualization\n",
    "\n",
    "### What Are Feature Maps?\n",
    "\n",
    "**Feature maps** (activation maps) are the outputs of convolutional layers. For a layer with $K$ filters:\n",
    "\n",
    "$$\\mathbf{A}^k = \\sigma(\\mathbf{W}^k * \\mathbf{X} + b^k), \\quad k = 1, \\ldots, K$$\n",
    "\n",
    "Each feature map $\\mathbf{A}^k$ highlights regions where specific patterns are detected.\n",
    "\n",
    "### Interpretation by Layer\n",
    "\n",
    "| Layer | Feature Map Shows | Spatial Resolution |\n",
    "|-------|------------------|-------------------|\n",
    "| `conv1` | Edges, color blobs | High (close to input) |\n",
    "| `conv2` | Texture patterns | Medium |\n",
    "| `conv3` | Object parts | Low (more abstract) |\n",
    "\n",
    "### Why Visualize Feature Maps?\n",
    "\n",
    "1. **Debugging**: Verify the network is learning meaningful features\n",
    "2. **Interpretation**: Understand *where* the network focuses attention\n",
    "3. **Research**: Analyze the feature hierarchy in novel architectures\n",
    "\n",
    "> *\"Early layers detect low-level features (edges, colors, textures). Later layers detect high-level concepts (object parts, shapes).\"* \u2014 Zeiler & Fergus, 2014\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc452a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample image for visualization\n",
    "sample_img, sample_label = sample_images[0:1], sample_labels[0]\n",
    "print(f\"Analyzing image of: {classes[sample_label]}\")\n",
    "\n",
    "# Register hooks to capture activations\n",
    "activations = {}\n",
    "\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activations[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Register hooks on each conv layer\n",
    "model.conv1.register_forward_hook(get_activation('conv1'))\n",
    "model.conv2.register_forward_hook(get_activation('conv2'))\n",
    "model.conv3.register_forward_hook(get_activation('conv3'))\n",
    "\n",
    "# Forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    _ = model(sample_img.to(device))\n",
    "\n",
    "# Visualize feature maps from each layer\n",
    "for layer_name in ['conv1', 'conv2', 'conv3']:\n",
    "    acts = activations[layer_name].cpu()\n",
    "    print(f\"\\n{layer_name} output shape: {acts.shape}\")\n",
    "    visualize.visualize_feature_maps(acts, num_maps=16, title=f\"Feature Maps: {layer_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733690e0",
   "metadata": {},
   "source": [
    "## 5. Filter (Kernel) Visualization\n",
    "\n",
    "### Theory: What Filters Learn\n",
    "\n",
    "Convolutional filters are the **learned parameters** $\\mathbf{W} \\in \\mathbb{R}^{C_{out} \\times C_{in} \\times k \\times k}$.\n",
    "\n",
    "For the **first layer** (RGB input with $C_{in}=3$), each filter can be visualized as a small RGB image showing what pattern the filter responds to.\n",
    "\n",
    "### Trained vs. Random Filters\n",
    "\n",
    "| Filter Type | Appearance | Interpretation |\n",
    "|-------------|-----------|----------------|\n",
    "| **Random (untrained)** | Noise-like | No meaningful patterns |\n",
    "| **Trained** | Structure | Edge detectors, color selectivity |\n",
    "\n",
    "### Common First-Layer Filter Types\n",
    "\n",
    "After training, first-layer filters typically include:\n",
    "- **Gabor-like filters**: Oriented edges at various angles\n",
    "- **Color-opponent filters**: Red-green, blue-yellow channels\n",
    "- **Gradient filters**: Similar to Sobel operators\n",
    "\n",
    "This emergence is consistent across different CNN architectures and datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f834a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize learned filters from the first conv layer\n",
    "print(\"First Layer Filters (RGB - what patterns is the model looking for?):\")\n",
    "visualize.visualize_filters(model.conv1, num_filters=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212f01ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with random (untrained) filters\n",
    "random_model = models.load_simple_cnn(num_classes=10)  # Fresh model\n",
    "print(\"Random (Untrained) Filters:\")\n",
    "visualize.visualize_filters(random_model.conv1, num_filters=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18d5936",
   "metadata": {},
   "source": [
    "## 6. Gradient-Based Interpretability: Saliency Maps\n",
    "\n",
    "### Theoretical Foundation\n",
    "\n",
    "**Saliency maps** (also called *vanilla gradients* or *input attribution*) identify which input pixels most influence the model's prediction by computing:\n",
    "\n",
    "$$S = \\left| \\frac{\\partial y^c}{\\partial \\mathbf{x}} \\right|$$\n",
    "\n",
    "where:\n",
    "- $y^c$ = Pre-softmax score for target class $c$\n",
    "- $\\mathbf{x}$ = Input image pixels\n",
    "- $S$ = Saliency map (same spatial dimensions as input)\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "1. **Forward pass**: Compute prediction $y^c$\n",
    "2. **Backward pass**: Compute gradient $\\nabla_{\\mathbf{x}} y^c$\n",
    "3. **Aggregate channels**: Take max or mean across RGB\n",
    "4. **Absolute value**: $S = |\\nabla_{\\mathbf{x}} y^c|$\n",
    "\n",
    "### Mathematical Interpretation\n",
    "\n",
    "The gradient represents the **local sensitivity** of the output to input perturbations:\n",
    "\n",
    "$$\\Delta y^c \\approx \\nabla_{\\mathbf{x}} y^c \\cdot \\Delta \\mathbf{x}$$\n",
    "\n",
    "High gradient magnitude \u2192 Small input change causes large output change \u2192 **Important pixel**.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- **Noisy**: Gradients can be high-frequency and visually noisy\n",
    "- **Saturation**: ReLU gradients are zero where activations are negative\n",
    "- **Local**: Only captures first-order effects\n",
    "\n",
    "Advanced methods like **Integrated Gradients** and **SmoothGrad** address these issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3b03ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute saliency map for our sample image\n",
    "model.eval()\n",
    "saliency = visualize.compute_saliency_map(\n",
    "    model=model,\n",
    "    input_tensor=sample_img,\n",
    "    target_class=sample_label.item(),\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Visualize\n",
    "visualize.visualize_saliency(\n",
    "    input_tensor=sample_images[0],\n",
    "    saliency_map=saliency,\n",
    "    title=f\"Saliency Map for '{classes[sample_label]}'\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4240d63",
   "metadata": {},
   "source": [
    "## 7. Class Activation Mapping (Grad-CAM)\n",
    "\n",
    "### Theoretical Foundation\n",
    "\n",
    "**Gradient-weighted Class Activation Mapping (Grad-CAM)** produces coarse localization maps highlighting important regions for a target class $c$. Unlike saliency maps, Grad-CAM operates on **feature maps** rather than input pixels.\n",
    "\n",
    "### The Grad-CAM Formula\n",
    "\n",
    "**Step 1: Compute importance weights**\n",
    "\n",
    "$$\\alpha_k^c = \\underbrace{\\frac{1}{Z} \\sum_i \\sum_j}_{\\text{global avg pool}} \\frac{\\partial y^c}{\\partial A_{ij}^k}$$\n",
    "\n",
    "where:\n",
    "- $A^k$ = Feature map $k$ from the target convolutional layer\n",
    "- $y^c$ = Score for class $c$ (before softmax)\n",
    "- $Z$ = Number of pixels in the feature map\n",
    "\n",
    "**Step 2: Weighted combination with ReLU**\n",
    "\n",
    "$$L_{\\text{Grad-CAM}}^c = \\text{ReLU}\\left(\\sum_k \\alpha_k^c A^k\\right)$$\n",
    "\n",
    "The ReLU ensures we only visualize features with **positive influence** on the target class.\n",
    "\n",
    "### Why Grad-CAM Works\n",
    "\n",
    "| Property | Saliency Maps | Grad-CAM |\n",
    "|----------|--------------|----------|\n",
    "| Resolution | Pixel-level (high) | Feature-level (coarse) |\n",
    "| Noise | High-frequency noise | Smooth heatmap |\n",
    "| Interpretation | Which pixels matter | Which *regions* matter |\n",
    "| Class-discriminative | Yes | Yes |\n",
    "\n",
    "### Reference\n",
    "\n",
    "> Selvaraju et al., \"Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization\", ICCV 2017.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9b2043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Grad-CAM for our trained SimpleCNN\n",
    "# We need to get the last conv layer\n",
    "target_layer = model.conv3\n",
    "\n",
    "gradcam = visualize.GradCAM(model, target_layer)\n",
    "heatmap = gradcam(\n",
    "    input_tensor=sample_img,\n",
    "    target_class=sample_label.item(),\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Visualize\n",
    "visualize.visualize_gradcam(\n",
    "    input_tensor=sample_images[0],\n",
    "    heatmap=heatmap,\n",
    "    predicted_class=classes[sample_label],\n",
    "    title=f\"Grad-CAM: Why is this a '{classes[sample_label]}'?\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0b1bda",
   "metadata": {},
   "source": [
    "## 8. Deep Interpretability with InceptionV1\n",
    "\n",
    "### InceptionV1 (GoogLeNet) Architecture\n",
    "\n",
    "InceptionV1 introduced the **Inception module**, which processes input through parallel convolutions of different sizes:\n",
    "\n",
    "```\n",
    "        Input\n",
    "          \u2502\n",
    "    \u250c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "    \u2502     \u2502     \u2502     \u2502\n",
    "   1\u00d71   3\u00d73   5\u00d75  MaxPool\n",
    "    \u2502     \u2502     \u2502     \u2502\n",
    "    \u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "          \u2502\n",
    "     Concatenate\n",
    "```\n",
    "\n",
    "### Why InceptionV1 for Interpretability?\n",
    "\n",
    "1. **Multi-scale features**: Parallel branches capture patterns at different spatial scales\n",
    "2. **Rich representations**: Concatenation creates diverse feature maps\n",
    "3. **Distill.pub research**: Extensively studied for feature visualization (Olah et al., 2017)\n",
    "4. **Pretrained on ImageNet**: 1000 classes, robust feature learning\n",
    "\n",
    "### Network Depth and Interpretability\n",
    "\n",
    "| Layer | Example Content | Interpretability |\n",
    "|-------|----------------|------------------|\n",
    "| `conv1` | Gabor-like edges | Easy to interpret |\n",
    "| `inception3a` | Texture combinations | Moderate |\n",
    "| `inception4a` | Object parts | Harder to interpret |\n",
    "| `inception5b` | Full objects | Most abstract |\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "> The deeper the layer, the more **class-specific** and **abstract** the features become. Grad-CAM on `inception5b` highlights semantic regions relevant to the predicted class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35360373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained InceptionV1\n",
    "inception = models.load_inception_v1(pretrained=True).to(device)\n",
    "print(\"\u2705 InceptionV1 (GoogLeNet) loaded!\")\n",
    "\n",
    "# Download a sample image for InceptionV1 demo\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "sample_url = \"https://images.unsplash.com/photo-1543466835-00a7907e9de1?ixlib=rb-4.0.3&q=85&fm=jpg&crop=entropy&cs=srgb&w=512\"\n",
    "response = requests.get(sample_url)\n",
    "sample_pil_image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(sample_pil_image)\n",
    "plt.axis('off')\n",
    "plt.title(\"Input Image for InceptionV1\")\n",
    "plt.show()\n",
    "\n",
    "# Preprocess image for Inception (224x224)\n",
    "inception_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Apply transform to PIL image\n",
    "inception_input = inception_transform(sample_pil_image).unsqueeze(0).to(device)\n",
    "\n",
    "# Get prediction\n",
    "with torch.no_grad():\n",
    "    output = inception(inception_input)\n",
    "    probs = F.softmax(output[0], dim=0)\n",
    "    top5_prob, top5_idx = probs.topk(5)\n",
    "\n",
    "# Load ImageNet labels\n",
    "import json\n",
    "import urllib.request\n",
    "LABELS_URL = \"https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\"\n",
    "with urllib.request.urlopen(LABELS_URL) as url:\n",
    "    imagenet_labels = json.loads(url.read().decode())\n",
    "\n",
    "print(\"\ud83c\udfc6 Top 5 Predictions:\")\n",
    "for i, (prob, idx) in enumerate(zip(top5_prob, top5_idx)):\n",
    "    print(f\"  {i+1}. {imagenet_labels[idx]}: {prob:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee172928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize InceptionV1 first layer filters\n",
    "print(\"InceptionV1 First Layer Filters:\")\n",
    "visualize.visualize_filters(inception.conv1.conv, num_filters=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47456bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grad-CAM on InceptionV1\n",
    "# Target the last conv layer before the GAP (inception5b)\n",
    "inception_target_layer = inception.inception5b\n",
    "\n",
    "inception_gradcam = visualize.GradCAM(inception, inception_target_layer.branch4[1])\n",
    "inception_heatmap = inception_gradcam(\n",
    "    input_tensor=inception_input,\n",
    "    target_class=top5_idx[0].item(),\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Visualize\n",
    "visualize.visualize_gradcam(\n",
    "    input_tensor=inception_input.squeeze(0),\n",
    "    heatmap=inception_heatmap,\n",
    "    predicted_class=imagenet_labels[top5_idx[0]],\n",
    "    title=f\"Grad-CAM on InceptionV1\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da71a247",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "### Methods Covered\n",
    "\n",
    "| Technique | Question Answered | Mathematical Core | Output |\n",
    "|-----------|------------------|-------------------|--------|\n",
    "| **Feature Maps** | Where did the model detect patterns? | $A^k = \\sigma(W^k * X + b^k)$ | Activation grids |\n",
    "| **Filter Visualization** | What patterns is it looking for? | Visualize $W^k$ directly | Kernel weights as images |\n",
    "| **Saliency Maps** | Which pixels influenced the prediction? | $S = \\|\\nabla_x y^c\\|$ | Gradient-based heatmap |\n",
    "| **Grad-CAM** | Which *regions* were important? | $L^c = \\text{ReLU}(\\sum_k \\alpha_k^c A^k)$ | Coarse localization heatmap |\n",
    "\n",
    "### The Interpretability Stack\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502     High-Level Explanations      \u2502  \u2190 Grad-CAM, CAM\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502       Feature Attribution        \u2502  \u2190 Saliency, Integrated Gradients\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502     Internal Representations     \u2502  \u2190 Feature maps, filter visualization\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502         Model Parameters         \u2502  \u2190 Weight analysis\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "### Practical Applications\n",
    "\n",
    "1. **Model debugging**: Find spurious correlations (e.g., model focusing on background)\n",
    "2. **Trust building**: Show users *why* a medical AI made a diagnosis\n",
    "3. **Research**: Understand how CNNs develop hierarchical representations\n",
    "4. **Adversarial robustness**: Detect when models rely on fragile features\n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- [Distill.pub Feature Visualization](https://distill.pub/2017/feature-visualization/)\n",
    "- [Grad-CAM Paper (ICCV 2017)](https://arxiv.org/abs/1610.02391)\n",
    "- [Captum Library](https://captum.ai/) \u2014 PyTorch interpretability toolkit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907dc00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83c\udf89 Congratulations! You've completed the Vision Interpretability tutorial!\")\n",
    "print(\"\\n\ud83d\udcda Resources:\")\n",
    "print(\"  - Grad-CAM Paper: https://arxiv.org/abs/1610.02391\")\n",
    "print(\"  - Captum (PyTorch Interpretability): https://captum.ai/\")\n",
    "print(\"  - ImageNette Dataset: https://huggingface.co/datasets/frgfm/imagenette\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}