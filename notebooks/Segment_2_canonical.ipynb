{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Segment 2 ‚Äî Activation Maximization (InceptionV1)\n",
    "\n",
    "This notebook generates **activation maximization images** for every neuron\n",
    "in a chosen InceptionV1 layer using [Lucent](https://github.com/greentfrapp/lucent).\n",
    "\n",
    "## What this notebook does\n",
    "1. Loads a pretrained InceptionV1 model\n",
    "2. For each neuron in a layer, optimizes a random image to maximally activate it\n",
    "3. Saves the resulting visualization as a `.png` file (lossless)\n",
    "4. Logs all metrics and images to [Weights & Biases](https://wandb.ai)\n",
    "\n",
    "## Robustness features\n",
    "- **Resume capability** ‚Äî re-running skips already-completed neurons\n",
    "- **GPU memory management** ‚Äî periodic cleanup prevents OOM on long runs\n",
    "- **Error isolation** ‚Äî a single neuron failure won‚Äôt crash the entire run\n",
    "- **Disk error handling** ‚Äî graceful skip if disk write fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 2: Environment Setup\n",
    "# =============================================================================\n",
    "# Detects whether we are running on Google Colab or locally.\n",
    "# Sets the CUDA memory allocator to use expandable segments, which prevents\n",
    "# GPU memory fragmentation during long runs (800+ neuron iterations).\n",
    "#\n",
    "# ROBUSTNESS NOTE (#2 ‚Äî CUDA Fragmentation):\n",
    "#   expandable_segments:True must be set BEFORE importing torch.\n",
    "#   Without this, after ~400-500 neurons the allocator may report\n",
    "#   \"CUDA out of memory\" even though nvidia-smi shows free memory.\n",
    "# =============================================================================\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# --- CUDA allocator config (must be before torch import) ---\n",
    "# This tells PyTorch to use expandable memory segments instead of\n",
    "# fixed-size blocks. It prevents the \"reserved but unallocated\" OOM\n",
    "# pattern that appears in long-running loops.\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# --- Environment detection ---\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Colab-specific: install lucent if not available\n",
    "    pass\n",
    "else:\n",
    "    # Local setup: add project source directory to Python path\n",
    "    from pathlib import Path\n",
    "\n",
    "    project_root = Path.cwd().parent  # notebooks/ ‚Üí project root\n",
    "    sys.path.insert(0, str(project_root / \"src\"))\n",
    "    print(f\"‚úÖ Local setup complete (project root: {project_root})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 3: Imports & Configuration\n",
    "# =============================================================================\n",
    "# All imports are grouped by category (PEP-8 style):\n",
    "#   1. Standard library\n",
    "#   2. Third-party libraries\n",
    "#   3. Project-specific / Lucent\n",
    "#\n",
    "# The CONFIG dictionary centralizes every tunable parameter so you\n",
    "# only need to edit one place to change the run.\n",
    "# =============================================================================\n",
    "\n",
    "# --- Standard library ---\n",
    "import gc                    # Garbage collector ‚Äî forces Python to free memory\n",
    "import time                  # Wall-clock timing for throughput measurement\n",
    "\n",
    "# --- Third-party ---\n",
    "import torch                 # PyTorch ‚Äî GPU tensor computation\n",
    "import numpy as np           # NumPy ‚Äî array operations for image processing\n",
    "import matplotlib.pyplot as plt  # Matplotlib ‚Äî (optional) for inline plots\n",
    "from pathlib import Path     # Pathlib ‚Äî cross-platform file path handling\n",
    "from PIL import Image        # Pillow ‚Äî PNG/JPEG image saving\n",
    "from tqdm.auto import tqdm   # tqdm ‚Äî progress bar with ETA and throughput\n",
    "from dotenv import load_dotenv  # dotenv ‚Äî loads .env file for API keys\n",
    "\n",
    "# --- Lucent (activation maximization library) ---\n",
    "from lucent.optvis import render, param   # render_vis + image parameterization\n",
    "from lucent.modelzoo import inceptionv1   # Pretrained InceptionV1 model\n",
    "\n",
    "# --- Load environment variables (local only) ---\n",
    "# The .env file should contain your WANDB_API_KEY\n",
    "if not IN_COLAB:\n",
    "    load_dotenv(project_root / \".env\")\n",
    "\n",
    "# --- Device selection ---\n",
    "# Uses GPU if available, otherwise falls back to CPU.\n",
    "# For 832+ neurons at 512px, a GPU is strongly recommended (~30s/neuron on GPU\n",
    "# vs ~10min/neuron on CPU).\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üñ•Ô∏è  Using device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"   GPU name: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"   GPU memory: {torch.cuda.get_device_properties(0).total_memory / 2**30:.1f} GB\")\n",
    "\n",
    "# --- Configuration dictionary ---\n",
    "# Edit these values to customize your run.\n",
    "# All other cells read from this dictionary.\n",
    "CONFIG = {\n",
    "    # Which layer to visualize (e.g., \"mixed4a\", \"mixed5a\", \"mixed5b\")\n",
    "    \"layer\": \"mixed5b\",\n",
    "\n",
    "    # Which neurons to process ‚Äî range(0, 832) for mixed5a, range(0, 1024) for mixed5b\n",
    "    \"neurons\": list(range(0, 1024)),\n",
    "\n",
    "    # Output image resolution in pixels (higher = more detail, more memory)\n",
    "    \"image_size\": 512,\n",
    "\n",
    "    # Number of optimization steps per neuron (higher = sharper features)\n",
    "    \"num_steps\": 1024,\n",
    "\n",
    "    # How often to flush GPU memory (every N neurons)\n",
    "    # Lower = more frequent cleanup = slightly slower but safer\n",
    "    \"memory_cleanup_interval\": 50,\n",
    "\n",
    "    # Weights & Biases project/run naming\n",
    "    \"wandb_project\": \"vision-interpretability\",\n",
    "    \"wandb_run_name\": \"mixed5b-actmax-512px\",\n",
    "\n",
    "    # Image format: \"png\" for lossless, \"jpg\" for smaller files\n",
    "    \"image_format\": \"png\",\n",
    "\n",
    "    # PNG compress level (0-9, lower = faster but larger files)\n",
    "    # Only used when image_format is \"png\"\n",
    "    \"png_compress_level\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 4: Auto-Increment Run ID\n",
    "# =============================================================================\n",
    "# Creates a sequential run ID (001, 002, 003, ...) by scanning the results\n",
    "# directory for existing run folders.\n",
    "#\n",
    "# ROBUSTNESS NOTE (#9 ‚Äî Multi-Layer Runs):\n",
    "#   Each run gets its own numbered directory, and within it, each layer\n",
    "#   gets a subdirectory. This means you can run mixed5a then mixed5b\n",
    "#   under the same run ID, or give each its own run ID.\n",
    "#\n",
    "# Directory structure:\n",
    "#   results/segment_2_activation_max/\n",
    "#   ‚îú‚îÄ‚îÄ 001/\n",
    "#   ‚îÇ   ‚îî‚îÄ‚îÄ mixed5a/\n",
    "#   ‚îÇ       ‚îú‚îÄ‚îÄ 0.png\n",
    "#   ‚îÇ       ‚îú‚îÄ‚îÄ 1.png\n",
    "#   ‚îÇ       ‚îî‚îÄ‚îÄ ...\n",
    "#   ‚îî‚îÄ‚îÄ 002/\n",
    "#       ‚îî‚îÄ‚îÄ mixed5b/\n",
    "#           ‚îî‚îÄ‚îÄ ...\n",
    "# =============================================================================\n",
    "\n",
    "# --- Choose base directory based on environment ---\n",
    "if IN_COLAB:\n",
    "    _results_base = Path(\"/content/drive/MyDrive/activation_max_results\")\n",
    "else:\n",
    "    _results_base = project_root / \"notebooks\" / \"results\" / \"segment_2_activation_max\"\n",
    "\n",
    "# --- Create base directory if it doesn't exist ---\n",
    "_results_base.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Find the next sequential run ID ---\n",
    "# Scan for folders named \"001\", \"002\", etc. and pick the next number.\n",
    "_existing_ids = [\n",
    "    int(p.name)\n",
    "    for p in _results_base.iterdir()\n",
    "    if p.is_dir() and p.name.isdigit()\n",
    "]\n",
    "run_id = f\"{max(_existing_ids) + 1:03d}\" if _existing_ids else \"001\"\n",
    "\n",
    "# --- Create the output directory for this run + layer ---\n",
    "save_dir = _results_base / run_id / CONFIG[\"layer\"]\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Run ID:    {run_id}\")\n",
    "print(f\"üìÅ Save dir:  {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 5: Initialize Weights & Biases\n",
    "# =============================================================================\n",
    "# W&B tracks metrics, images, and system stats for every run.\n",
    "# You can view your runs at: https://wandb.ai/<your-username>/vision-interpretability\n",
    "#\n",
    "# ROBUSTNESS NOTE (#6 ‚Äî W&B Network Resilience):\n",
    "#   init_timeout=120 gives W&B 2 minutes to connect (default is 60s).\n",
    "#   If the network drops mid-run, W&B buffers data locally and syncs\n",
    "#   when reconnected ‚Äî no data is lost.\n",
    "# =============================================================================\n",
    "\n",
    "import wandb\n",
    "\n",
    "wandb.init(\n",
    "    project=CONFIG[\"wandb_project\"],\n",
    "    name=f\"{CONFIG['wandb_run_name']}-run{run_id}\",\n",
    "    config=CONFIG,\n",
    "    settings=wandb.Settings(\n",
    "        init_timeout=120,  # Allow 2 min for init on slow networks\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 6: Load Pretrained InceptionV1\n",
    "# =============================================================================\n",
    "# Loads InceptionV1 with ImageNet weights and freezes all parameters.\n",
    "# We freeze parameters because we are NOT training the model ‚Äî we only\n",
    "# need the forward pass to measure neuron activations.\n",
    "# =============================================================================\n",
    "\n",
    "# --- Load and configure model ---\n",
    "model = inceptionv1(pretrained=True)  # Download ImageNet weights\n",
    "model = model.to(device)              # Move to GPU (if available)\n",
    "model = model.eval()                  # Set to evaluation mode (disables dropout, etc.)\n",
    "\n",
    "# --- Freeze all parameters ---\n",
    "# This saves memory because PyTorch won't store gradients for frozen params.\n",
    "for p in model.parameters():\n",
    "    p.requires_grad_(False)\n",
    "\n",
    "print(f\"‚úÖ InceptionV1 loaded on {device} (all parameters frozen)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Maximization Function\n",
    "\n",
    "The function below optimizes a random noise image so that it **maximally activates**\n",
    "a specific neuron in the network. This is done via gradient ascent on the input image\n",
    "(not the model weights).\n",
    "\n",
    "**How it works:**\n",
    "1. Start with a random image parameterized in Fourier space (`fft=True`)\n",
    "2. Run it through the model and measure the target neuron‚Äôs activation\n",
    "3. Compute gradients of the activation with respect to the image pixels\n",
    "4. Update the image to increase the activation\n",
    "5. Repeat for `num_steps` iterations\n",
    "\n",
    "The `decorrelate=True` flag applies a color decorrelation transform that\n",
    "produces more natural-looking colors in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 8: Activation Maximization Function\n",
    "# =============================================================================\n",
    "# This function wraps Lucent's render_vis() to generate one activation\n",
    "# maximization image for a single neuron.\n",
    "#\n",
    "# Parameters:\n",
    "#   model       ‚Äî the pretrained InceptionV1 model\n",
    "#   layer_name  ‚Äî which layer to target (e.g., \"mixed5a\")\n",
    "#   neuron_id   ‚Äî which neuron/channel in that layer (e.g., 42)\n",
    "#   image_size  ‚Äî output resolution in pixels (default 512)\n",
    "#   num_steps   ‚Äî number of optimization iterations (default 1024)\n",
    "#\n",
    "# Returns:\n",
    "#   numpy array of shape (H, W, 3) with values in [0, 1]\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def activation_maximization(\n",
    "    model,\n",
    "    layer_name,\n",
    "    neuron_id,\n",
    "    image_size=512,\n",
    "    num_steps=1024,\n",
    "):\n",
    "    \"\"\"Generate an activation maximization image for a single neuron.\n",
    "\n",
    "    Uses Lucent's render_vis with FFT parameterization and color\n",
    "    decorrelation for visually interpretable results.\n",
    "\n",
    "    Args:\n",
    "        model: Pretrained PyTorch model (e.g., InceptionV1).\n",
    "        layer_name: Target layer name (e.g., \"mixed5a\").\n",
    "        neuron_id: Target neuron/channel index within the layer.\n",
    "        image_size: Output image resolution in pixels. Default 512.\n",
    "        num_steps: Number of gradient ascent steps. Default 1024.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Image array of shape (H, W, 3), values in [0, 1].\n",
    "    \"\"\"\n",
    "    # Build the objective string that Lucent understands\n",
    "    # Format: \"layer_name:neuron_index\"\n",
    "    objective = f\"{layer_name}:{neuron_id}\"\n",
    "\n",
    "    # Define the image parameterization:\n",
    "    #   fft=True        ‚Üí optimize in Fourier space (smoother results)\n",
    "    #   decorrelate=True ‚Üí apply color decorrelation (more natural colors)\n",
    "    param_f = lambda: param.image(\n",
    "        image_size,\n",
    "        fft=True,\n",
    "        decorrelate=True,\n",
    "    )\n",
    "\n",
    "    # Run the optimization loop\n",
    "    #   thresholds=(num_steps,) ‚Üí only return the final image, not intermediates\n",
    "    #   show_image=False        ‚Üí don't display inline (we save to disk instead)\n",
    "    #   verbose=False           ‚Üí suppress Lucent's own progress output\n",
    "    images = render.render_vis(\n",
    "        model,\n",
    "        objective,\n",
    "        param_f=param_f,\n",
    "        thresholds=(num_steps,),\n",
    "        show_image=False,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    # images is a list of [batch of images] per threshold\n",
    "    # images[0] = results at our single threshold\n",
    "    # images[0][0] = first (and only) image in the batch\n",
    "    return images[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate & Save All Neuron Visualizations\n",
    "\n",
    "The cell below is the **main execution loop**. It processes every neuron in\n",
    "`CONFIG[\"neurons\"]` and saves the resulting image to disk.\n",
    "\n",
    "### Robustness features built into this loop:\n",
    "| Feature | What it does |\n",
    "|---------|--------------|\n",
    "| **Resume** | Skips neurons that already have a `.png` file (lossless) saved |\n",
    "| **Memory cleanup** | Runs `gc.collect()` + `torch.cuda.empty_cache()` every 50 neurons |\n",
    "| **OOM recovery** | If a neuron fails, clears GPU memory before trying the next one |\n",
    "| **NaN/Inf guard** | Detects degenerate outputs and skips them |\n",
    "| **Disk error handling** | Catches write failures without crashing |\n",
    "| **Progress bar** | Shows ETA, speed, and failure count in real time |\n",
    "| **W&B logging** | Logs timing, images, and GPU memory to your dashboard |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 10: Generate & Save All Neuron Visualizations\n",
    "# =============================================================================\n",
    "# Main execution loop with full robustness for 7-8 hour runs.\n",
    "# See the markdown cell above for a summary of all safety features.\n",
    "# =============================================================================\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Timing accumulators ‚Äî track where time is spent across the run\n",
    "# -------------------------------------------------------------------------\n",
    "total_time = 0.0          # Will be computed at the end\n",
    "time_optimization = 0.0   # Cumulative time spent in render_vis()\n",
    "time_saving = 0.0         # Cumulative time spent writing image files\n",
    "neurons_completed = 0     # Count of successfully saved neurons\n",
    "neurons_failed = 0        # Count of neurons that errored or produced NaN\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# ROBUSTNESS #8 ‚Äî Resume after crash\n",
    "# -------------------------------------------------------------------------\n",
    "# Scan the save directory for image files that already exist.\n",
    "# If the notebook was interrupted at hour 5, restarting it will\n",
    "# seamlessly skip to the first uncompleted neuron.\n",
    "_ext = CONFIG[\"image_format\"]  # \"png\" or \"jpg\"\n",
    "_existing = {int(f.stem) for f in save_dir.glob(f\"*.{_ext}\")}\n",
    "neurons_to_process = [n for n in CONFIG[\"neurons\"] if n not in _existing]\n",
    "_skipped = len(CONFIG[\"neurons\"]) - len(neurons_to_process)\n",
    "\n",
    "if _skipped:\n",
    "    print(f\"‚è≠Ô∏è  Skipping {_skipped} already-completed neurons (found in {save_dir})\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Progress bar setup\n",
    "# -------------------------------------------------------------------------\n",
    "# tqdm provides a live progress bar with:\n",
    "#   - elapsed time and ETA\n",
    "#   - processing speed (neurons/sec)\n",
    "#   - custom postfix showing last/avg time and failure count\n",
    "pbar = tqdm(\n",
    "    neurons_to_process,\n",
    "    desc=f\"üß† {CONFIG['layer']}\",\n",
    "    unit=\"neuron\",\n",
    "    dynamic_ncols=True,\n",
    "    bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]\",\n",
    ")\n",
    "\n",
    "# Record the start time for total elapsed calculation\n",
    "t_start_total = time.time()\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Main loop ‚Äî one iteration per neuron\n",
    "# -------------------------------------------------------------------------\n",
    "for neuron_id in pbar:\n",
    "    t_neuron_start = time.time()\n",
    "\n",
    "    try:\n",
    "        # -----------------------------------------------------------------\n",
    "        # Step 1: Run activation maximization\n",
    "        # -----------------------------------------------------------------\n",
    "        # ROBUSTNESS #10 ‚Äî torch.no_grad() wrapper\n",
    "        # Lucent manages its own gradients internally. This wrapper\n",
    "        # prevents any accidental computational graph construction\n",
    "        # in our outer code, saving memory.\n",
    "        t_opt_start = time.time()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            img = activation_maximization(\n",
    "                model,\n",
    "                CONFIG[\"layer\"],\n",
    "                neuron_id,\n",
    "                image_size=CONFIG[\"image_size\"],\n",
    "                num_steps=CONFIG[\"num_steps\"],\n",
    "            )\n",
    "\n",
    "        opt_time = time.time() - t_opt_start\n",
    "        time_optimization += opt_time\n",
    "\n",
    "        # -----------------------------------------------------------------\n",
    "        # Step 2: Safety ‚Äî detach tensor if Lucent returned one\n",
    "        # -----------------------------------------------------------------\n",
    "        # ROBUSTNESS #3 ‚Äî Computational graph leak prevention\n",
    "        # Normally Lucent returns a numpy array, but as a safety net\n",
    "        # we check and detach if it's a tensor. This prevents the\n",
    "        # computational graph from being pinned in GPU memory.\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img = img.detach().cpu().numpy()\n",
    "\n",
    "        # -----------------------------------------------------------------\n",
    "        # Step 3: Check for NaN or Inf values\n",
    "        # -----------------------------------------------------------------\n",
    "        # ROBUSTNESS #4 ‚Äî NaN/Inf detection\n",
    "        # Some neurons may produce degenerate optimizations. Rather than\n",
    "        # saving a corrupt image, we log it and move on.\n",
    "        if np.any(np.isnan(img)) or np.any(np.isinf(img)):\n",
    "            print(f\"\\n‚ö†Ô∏è Neuron {neuron_id}: NaN/Inf detected, skipping save\")\n",
    "            wandb.log({\"neuron\": neuron_id, \"error\": \"NaN/Inf in output\"})\n",
    "            neurons_failed += 1\n",
    "            del img  # Free the bad array\n",
    "            continue\n",
    "\n",
    "        # -----------------------------------------------------------------\n",
    "        # Step 4: Convert and save to disk\n",
    "        # -----------------------------------------------------------------\n",
    "        # ROBUSTNESS #5 ‚Äî Disk I/O error handling\n",
    "        # Convert float [0,1] ‚Üí uint8 [0,255] and save as PNG (lossless).\n",
    "        # Wrapped in try/except to handle disk-full or permission errors.\n",
    "        t_save_start = time.time()\n",
    "        img_uint8 = (np.clip(img, 0, 1) * 255).astype(np.uint8)\n",
    "        save_path = save_dir / f\"{neuron_id}.{CONFIG['image_format']}\"\n",
    "\n",
    "        try:\n",
    "            # PNG: use compress_level for speed/size tradeoff (lossless)\n",
    "            # JPG: use quality=100 for minimum compression\n",
    "            if CONFIG[\"image_format\"] == \"png\":\n",
    "                Image.fromarray(img_uint8).save(\n",
    "                    save_path, compress_level=CONFIG[\"png_compress_level\"]\n",
    "                )\n",
    "            else:\n",
    "                Image.fromarray(img_uint8).save(save_path, quality=100)\n",
    "        except OSError as e:\n",
    "            print(f\"\\n‚ö†Ô∏è Neuron {neuron_id}: Disk write failed: {e}\")\n",
    "            wandb.log({\"neuron\": neuron_id, \"error\": f\"disk: {e}\"})\n",
    "            neurons_failed += 1\n",
    "            del img, img_uint8  # Clean up before continuing\n",
    "            continue\n",
    "\n",
    "        save_time = time.time() - t_save_start\n",
    "        time_saving += save_time\n",
    "\n",
    "        # -----------------------------------------------------------------\n",
    "        # Step 5: Record success\n",
    "        # -----------------------------------------------------------------\n",
    "        neuron_time = time.time() - t_neuron_start\n",
    "        neurons_completed += 1\n",
    "\n",
    "        # -----------------------------------------------------------------\n",
    "        # Step 6: Log metrics and image to W&B\n",
    "        # -----------------------------------------------------------------\n",
    "        wandb.log({\n",
    "            \"neuron\": neuron_id,\n",
    "            \"optimization_time_sec\": opt_time,\n",
    "            \"save_time_sec\": save_time,\n",
    "            \"total_neuron_time_sec\": neuron_time,\n",
    "            \"neurons_completed\": neurons_completed,\n",
    "            \"neurons_failed\": neurons_failed,\n",
    "            \"elapsed_total_sec\": time.time() - t_start_total,\n",
    "            \"image\": wandb.Image(\n",
    "                img_uint8,\n",
    "                caption=f\"{CONFIG['layer']}/n{neuron_id}\",\n",
    "            ),\n",
    "        })\n",
    "\n",
    "        # -----------------------------------------------------------------\n",
    "        # Step 7: Free large arrays\n",
    "        # -----------------------------------------------------------------\n",
    "        # ROBUSTNESS #1 ‚Äî Explicit memory lifecycle\n",
    "        # Delete references to the image arrays so Python can free them.\n",
    "        # This is especially important before gc.collect() runs.\n",
    "        del img, img_uint8\n",
    "\n",
    "    except Exception as e:\n",
    "        # -----------------------------------------------------------------\n",
    "        # Error recovery for ANY unexpected failure\n",
    "        # -----------------------------------------------------------------\n",
    "        # ROBUSTNESS #1 ‚Äî OOM recovery in except block\n",
    "        # If a neuron triggers an OOM or any other error, we:\n",
    "        #   1. Log it (don't crash)\n",
    "        #   2. Force garbage collection + CUDA cache clear\n",
    "        #   3. Continue to the next neuron\n",
    "        neurons_failed += 1\n",
    "        print(f\"\\n‚ö†Ô∏è Neuron {neuron_id} failed: {e}\")\n",
    "        wandb.log({\"neuron\": neuron_id, \"error\": str(e)})\n",
    "\n",
    "        gc.collect()\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        continue\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Periodic memory cleanup + GPU monitoring\n",
    "    # ---------------------------------------------------------------------\n",
    "    # ROBUSTNESS #1 + #7 ‚Äî Memory cleanup and leak detection\n",
    "    # Every `memory_cleanup_interval` neurons (default: 50), we:\n",
    "    #   1. Run Python garbage collection (frees unreferenced objects)\n",
    "    #   2. Release PyTorch's cached but unused GPU memory\n",
    "    #   3. Log current GPU memory stats to W&B for leak detection\n",
    "    #\n",
    "    # If gpu/memory_reserved_gb keeps climbing while gpu/memory_allocated_gb\n",
    "    # stays flat, you have a fragmentation issue.\n",
    "    if neurons_completed % CONFIG[\"memory_cleanup_interval\"] == 0:\n",
    "        gc.collect()\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "            wandb.log({\n",
    "                \"gpu/memory_allocated_gb\": torch.cuda.memory_allocated() / 2**30,\n",
    "                \"gpu/memory_reserved_gb\": torch.cuda.memory_reserved() / 2**30,\n",
    "                \"gpu/memory_peak_gb\": torch.cuda.max_memory_allocated() / 2**30,\n",
    "            })\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Update progress bar postfix\n",
    "    # ---------------------------------------------------------------------\n",
    "    avg_time = time_optimization / neurons_completed if neurons_completed else 0\n",
    "    pbar.set_postfix({\n",
    "        \"last\": f\"{neuron_time:.1f}s\",\n",
    "        \"avg\": f\"{avg_time:.1f}s\",\n",
    "        \"fail\": neurons_failed,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Cell 11: Run Summary & Cleanup\n",
    "# =============================================================================\n",
    "# Prints a human-readable summary of the completed run and logs\n",
    "# final aggregate metrics to W&B before closing the run.\n",
    "# =============================================================================\n",
    "\n",
    "# --- Calculate totals ---\n",
    "total_time = time.time() - t_start_total\n",
    "throughput = neurons_completed / total_time if total_time > 0 else 0\n",
    "\n",
    "# --- Print summary ---\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(f\"  ‚úÖ Run {run_id} Complete ‚Äî {CONFIG['layer']}\")\n",
    "print(f\"{'=' * 60}\")\n",
    "print(f\"  Neurons completed:  {neurons_completed}\")\n",
    "print(f\"  Neurons skipped:    {_skipped}\")\n",
    "print(f\"  Neurons failed:     {neurons_failed}\")\n",
    "print(f\"  Total time:         {total_time:.1f}s ({total_time / 3600:.1f}h)\")\n",
    "print(f\"  Throughput:         {throughput:.2f} neurons/sec\")\n",
    "print()\n",
    "\n",
    "# --- Timing breakdown ---\n",
    "print(f\"  ‚è±Ô∏è  Timing Breakdown:\")\n",
    "if total_time > 0:\n",
    "    print(f\"     Optimization:   {time_optimization:.1f}s \"\n",
    "          f\"({100 * time_optimization / total_time:.1f}%)\")\n",
    "    print(f\"     Saving:         {time_saving:.1f}s \"\n",
    "          f\"({100 * time_saving / total_time:.1f}%)\")\n",
    "\n",
    "# --- GPU memory info ---\n",
    "if device.type == \"cuda\":\n",
    "    peak_mem = torch.cuda.max_memory_allocated() / 2**30\n",
    "    print(f\"  üñ•Ô∏è  Peak GPU memory: {peak_mem:.2f} GB\")\n",
    "print()\n",
    "print(f\"  üìÅ Results saved to: {save_dir}\")\n",
    "\n",
    "# --- Log final summary to W&B ---\n",
    "wandb.log({\n",
    "    \"summary/neurons_completed\": neurons_completed,\n",
    "    \"summary/neurons_skipped\": _skipped,\n",
    "    \"summary/neurons_failed\": neurons_failed,\n",
    "    \"summary/total_time_sec\": total_time,\n",
    "    \"summary/throughput_neurons_per_sec\": throughput,\n",
    "    \"summary/time_optimization_sec\": time_optimization,\n",
    "    \"summary/time_saving_sec\": time_saving,\n",
    "})\n",
    "\n",
    "# --- Close W&B run ---\n",
    "wandb.finish()\n",
    "print(\"‚úÖ W&B run finished and synced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
