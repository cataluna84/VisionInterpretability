{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cataluna84/VisionInterpretability/blob/main/notebooks/cataluna84__segment_2_activation_max.ipynb)\n",
                "\n",
                "# Segment 2: Activation Maximization - Visualizing Neural Network Features\n",
                "\n",
                "## Introduction\n",
                "\n",
                "This notebook reproduces the **activation maximization** visualizations from the groundbreaking\n",
                "[Distill.pub Circuits](https://distill.pub/2020/circuits/) research thread, specifically:\n",
                "\n",
                "- [Zoom In: An Introduction to Circuits](https://distill.pub/2020/circuits/zoom-in/)\n",
                "- [An Overview of Early Vision in InceptionV1](https://distill.pub/2020/circuits/early-vision/)\n",
                "\n",
                "These papers demonstrate that neural networks learn **interpretable features** at the neuron level,\n",
                "and we can visualize what each neuron \"looks for\" using optimization-based techniques.\n",
                "\n",
                "---\n",
                "\n",
                "## Theoretical Background\n",
                "\n",
                "### What is Activation Maximization?\n",
                "\n",
                "**Activation maximization** is a feature visualization technique that generates an input image\n",
                "which maximally activates a specific neuron (or channel) in a neural network.\n",
                "\n",
                "#### The Core Idea\n",
                "\n",
                "Given a trained neural network $f$ with frozen weights, we want to find an input image $\\mathbf{x}^*$\n",
                "that maximizes the activation of a target neuron $a_{l,k}$ at layer $l$, channel $k$:\n",
                "\n",
                "$$\\mathbf{x}^* = \\arg\\max_{\\mathbf{x}} \\, a_{l,k}(f(\\mathbf{x}))$$\n",
                "\n",
                "Where $a_{l,k}(f(\\mathbf{x}))$ represents the mean activation of channel $k$ at layer $l$ when\n",
                "the network processes input $\\mathbf{x}$.\n",
                "\n",
                "#### Optimization Formulation\n",
                "\n",
                "In practice, we solve this as a gradient-based optimization problem. Starting from a random\n",
                "or noise image $\\mathbf{x}_0$, we iteratively update the image:\n",
                "\n",
                "$$\\mathbf{x}_{t+1} = \\mathbf{x}_t + \\eta \\cdot \\nabla_{\\mathbf{x}} \\, a_{l,k}(f(\\mathbf{x}_t))$$\n",
                "\n",
                "Where:\n",
                "- $\\eta$ is the learning rate\n",
                "- $\\nabla_{\\mathbf{x}}$ is the gradient with respect to the input image\n",
                "- We're performing **gradient ascent** (maximizing, not minimizing)\n",
                "\n",
                "#### Regularization for Interpretable Images\n",
                "\n",
                "Without regularization, the optimized images often contain high-frequency noise patterns\n",
                "that exploit the network but aren't human-interpretable. Common regularization techniques include:\n",
                "\n",
                "1. **Total Variation (TV) Loss**: Penalizes high-frequency variations\n",
                "   $$\\mathcal{L}_{TV}(\\mathbf{x}) = \\sum_{i,j} |x_{i+1,j} - x_{i,j}| + |x_{i,j+1} - x_{i,j}|$$\n",
                "\n",
                "2. **L2 Regularization**: Prevents pixel values from exploding\n",
                "   $$\\mathcal{L}_{L2}(\\mathbf{x}) = \\|\\mathbf{x}\\|_2^2$$\n",
                "\n",
                "3. **Transformation Robustness**: Apply random jitter, rotation, scaling during optimization\n",
                "\n",
                "4. **Decorrelated Color Space**: Optimize in a decorrelated color space (often Fourier-based)\n",
                "\n",
                "The final objective with regularization becomes:\n",
                "\n",
                "$$\\mathbf{x}^* = \\arg\\max_{\\mathbf{x}} \\left[ a_{l,k}(f(\\mathbf{x})) - \\lambda_{TV} \\mathcal{L}_{TV}(\\mathbf{x}) - \\lambda_{L2} \\mathcal{L}_{L2}(\\mathbf{x}) \\right]$$\n",
                "\n",
                "---\n",
                "\n",
                "### InceptionV1 Architecture Context\n",
                "\n",
                "InceptionV1 (GoogLeNet) uses **Inception modules** that apply multiple filter sizes in parallel.\n",
                "The layer naming convention follows:\n",
                "\n",
                "| Layer Name | Description | Typical Feature Complexity |\n",
                "|-----------|-------------|---------------------------|\n",
                "| `mixed3a/b` | Early layers | Edges, colors, textures |\n",
                "| `mixed4a/b/c/d/e` | Middle layers | Parts, shapes, patterns |\n",
                "| `mixed5a/b` | Later layers | Objects, high-level concepts |\n",
                "\n",
                "We focus on **`mixed4a`** — a transition layer where features become more semantically meaningful\n",
                "while still being relatively interpretable.\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Environment Setup\n",
                "\n",
                "We use **Lucent** — the PyTorch port of Google/OpenAI's **Lucid** library,\n",
                "created by the same team that wrote the Distill.pub Circuits articles."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\"\"\"Setup: Import libraries and configure environment.\n",
                "\n",
                "This cell imports all necessary dependencies for activation maximization\n",
                "visualization using the Lucent library (PyTorch port of Lucid).\n",
                "\"\"\"\n",
                "\n",
                "# Standard library imports\n",
                "import warnings\n",
                "from typing import Optional\n",
                "\n",
                "# Third-party imports\n",
                "import torch\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from matplotlib.gridspec import GridSpec\n",
                "from PIL import Image\n",
                "\n",
                "# Lucent imports for feature visualization\n",
                "from lucent.optvis import render, param, transform, objectives\n",
                "from lucent.modelzoo import inceptionv1\n",
                "\n",
                "# Suppress non-critical warnings for cleaner output\n",
                "warnings.filterwarnings('ignore', category=UserWarning)\n",
                "\n",
                "# Configure matplotlib for high-quality figures\n",
                "plt.rcParams.update({\n",
                "    'figure.figsize': (12, 8),\n",
                "    'figure.dpi': 100,\n",
                "    'axes.titlesize': 12,\n",
                "    'axes.labelsize': 10,\n",
                "    'font.family': 'sans-serif',\n",
                "})\n",
                "\n",
                "# Device configuration\n",
                "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {DEVICE}\")\n",
                "if DEVICE.type == 'cuda':\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Loading InceptionV1 Model\n",
                "\n",
                "Lucent provides a pre-trained InceptionV1 model with **correctly named layers**\n",
                "matching the Distill.pub convention (`mixed3a`, `mixed4a`, etc.)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\"\"\"Load the pre-trained InceptionV1 model from Lucent's model zoo.\n",
                "\n",
                "The model is loaded with ImageNet pre-trained weights and set to\n",
                "evaluation mode (frozen batch normalization, dropout disabled).\n",
                "\"\"\"\n",
                "\n",
                "\n",
                "def load_inceptionv1_model(device: torch.device) -> torch.nn.Module:\n",
                "    \"\"\"Load and prepare InceptionV1 model for feature visualization.\n",
                "\n",
                "    Args:\n",
                "        device: The torch device (cuda/cpu) to load the model onto.\n",
                "\n",
                "    Returns:\n",
                "        torch.nn.Module: The pre-trained InceptionV1 model in eval mode.\n",
                "\n",
                "    Notes:\n",
                "        The model uses layer names like 'mixed3a', 'mixed4a', etc.\n",
                "        which correspond to the Inception module outputs.\n",
                "    \"\"\"\n",
                "    print(\"Loading InceptionV1 model...\")\n",
                "    model = inceptionv1(pretrained=True)\n",
                "    model = model.to(device)\n",
                "    model = model.eval()\n",
                "\n",
                "    # Freeze all parameters (we only optimize the input image)\n",
                "    for param in model.parameters():\n",
                "        param.requires_grad = False\n",
                "\n",
                "    print(\"Model loaded successfully!\")\n",
                "    return model\n",
                "\n",
                "\n",
                "# Load the model\n",
                "model = load_inceptionv1_model(DEVICE)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Exploring the Model Architecture\n",
                "\n",
                "Let's examine the available layer names in InceptionV1 to understand\n",
                "where we can hook into for feature visualization."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\"\"\"Explore InceptionV1 architecture and available layer names.\n",
                "\n",
                "This helps us understand which layers we can target for\n",
                "activation maximization visualization.\n",
                "\"\"\"\n",
                "\n",
                "\n",
                "def print_model_layers(\n",
                "    model: torch.nn.Module,\n",
                "    max_depth: int = 1\n",
                ") -> None:\n",
                "    \"\"\"Print the top-level named modules of a PyTorch model.\n",
                "\n",
                "    Args:\n",
                "        model: The PyTorch model to inspect.\n",
                "        max_depth: Maximum depth of nested modules to show.\n",
                "\n",
                "    Returns:\n",
                "        None: Prints layer information to stdout.\n",
                "    \"\"\"\n",
                "    print(\"InceptionV1 Layer Structure:\")\n",
                "    print(\"=\" * 50)\n",
                "\n",
                "    for name, module in model.named_children():\n",
                "        # Get output shape info if available\n",
                "        module_type = type(module).__name__\n",
                "        print(f\"  {name:20s} -> {module_type}\")\n",
                "\n",
                "\n",
                "print_model_layers(model)\n",
                "\n",
                "print(\"\\n\" + \"=\" * 50)\n",
                "print(\"Key layers for visualization:\")\n",
                "print(\"  - mixed3a/b: Early vision (edges, textures)\")\n",
                "print(\"  - mixed4a/b/c/d/e: Mid-level features (shapes, patterns)\")\n",
                "print(\"  - mixed5a/b: High-level features (objects, parts)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Understanding mixed4a Layer\n",
                "\n",
                "Let's examine the `mixed4a` layer specifically to understand\n",
                "how many neurons (channels) it contains."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\"\"\"Analyze the mixed4a layer structure and channel count.\n",
                "\n",
                "mixed4a is an Inception module that concatenates outputs from\n",
                "multiple parallel convolution branches.\n",
                "\"\"\"\n",
                "\n",
                "\n",
                "def get_layer_output_channels(\n",
                "    model: torch.nn.Module,\n",
                "    layer_name: str,\n",
                "    input_size: tuple = (1, 3, 224, 224)\n",
                ") -> int:\n",
                "    \"\"\"Get the number of output channels for a specific layer.\n",
                "\n",
                "    Args:\n",
                "        model: The PyTorch model.\n",
                "        layer_name: Name of the layer to analyze.\n",
                "        input_size: Input tensor size for forward pass.\n",
                "\n",
                "    Returns:\n",
                "        int: Number of output channels at the specified layer.\n",
                "\n",
                "    Notes:\n",
                "        Uses a forward hook to capture the layer's output shape.\n",
                "    \"\"\"\n",
                "    output_shape = None\n",
                "\n",
                "    def hook_fn(module, input, output):\n",
                "        nonlocal output_shape\n",
                "        output_shape = output.shape\n",
                "\n",
                "    # Find and register hook on the target layer\n",
                "    for name, module in model.named_modules():\n",
                "        if name == layer_name:\n",
                "            handle = module.register_forward_hook(hook_fn)\n",
                "            break\n",
                "    else:\n",
                "        raise ValueError(f\"Layer '{layer_name}' not found in model\")\n",
                "\n",
                "    # Run forward pass with dummy input\n",
                "    with torch.no_grad():\n",
                "        dummy_input = torch.randn(input_size).to(DEVICE)\n",
                "        model(dummy_input)\n",
                "\n",
                "    handle.remove()\n",
                "    return output_shape[1]  # Channel dimension\n",
                "\n",
                "\n",
                "# Analyze mixed4a\n",
                "TARGET_LAYER = 'mixed4a'\n",
                "num_channels = get_layer_output_channels(model, TARGET_LAYER)\n",
                "\n",
                "print(f\"Layer: {TARGET_LAYER}\")\n",
                "print(f\"Number of channels (neurons): {num_channels}\")\n",
                "print(f\"We will visualize the first 10 neurons (indices 0-9)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 3. Single Neuron Visualization\n",
                "\n",
                "Before generating all 10 visualizations, let's understand the process\n",
                "by visualizing a single neuron in detail.\n",
                "\n",
                "### The render_vis Function\n",
                "\n",
                "Lucent's `render_vis` function handles all the optimization details:\n",
                "\n",
                "```python\n",
                "render.render_vis(\n",
                "    model,           # The neural network\n",
                "    objective,       # What to maximize (e.g., \"mixed4a:0\")\n",
                "    param_f=None,    # Image parameterization (default: FFT-based)\n",
                "    optimizer=None,  # Optimizer (default: Adam)\n",
                "    transforms=None, # Data augmentation for robustness\n",
                "    thresholds=(512,), # Optimization steps\n",
                "    verbose=False,   # Print progress\n",
                "    show_image=True, # Display result\n",
                ")\n",
                "```\n",
                "\n",
                "The objective string format is `\"layer_name:channel_index\"`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\"\"\"Demonstrate activation maximization for a single neuron.\n",
                "\n",
                "This cell visualizes neuron 0 of mixed4a layer using Lucent's\n",
                "render_vis function with default settings.\n",
                "\"\"\"\n",
                "\n",
                "\n",
                "def visualize_single_neuron(\n",
                "    model: torch.nn.Module,\n",
                "    layer_name: str,\n",
                "    neuron_idx: int,\n",
                "    image_size: int = 128,\n",
                "    num_steps: int = 512,\n",
                "    show: bool = True\n",
                ") -> np.ndarray:\n",
                "    \"\"\"Generate activation maximization visualization for a single neuron.\n",
                "\n",
                "    Uses gradient ascent to find an input image that maximally activates\n",
                "    the specified neuron in the network.\n",
                "\n",
                "    Args:\n",
                "        model: Pre-trained neural network in eval mode.\n",
                "        layer_name: Name of the layer containing the target neuron.\n",
                "        neuron_idx: Index of the neuron (channel) to visualize.\n",
                "        image_size: Output image resolution (square).\n",
                "        num_steps: Number of optimization iterations.\n",
                "        show: Whether to display the result.\n",
                "\n",
                "    Returns:\n",
                "        np.ndarray: The generated visualization image (H, W, 3).\n",
                "\n",
                "    Notes:\n",
                "        - Uses FFT-based image parameterization for smooth results\n",
                "        - Applies standard transformation robustness (jitter, scale, rotate)\n",
                "        - More steps generally produce clearer features\n",
                "    \"\"\"\n",
                "    # Define the optimization objective\n",
                "    objective = f\"{layer_name}:{neuron_idx}\"\n",
                "\n",
                "    print(f\"Generating visualization for {objective}...\")\n",
                "    print(f\"  Image size: {image_size}x{image_size}\")\n",
                "    print(f\"  Optimization steps: {num_steps}\")\n",
                "\n",
                "    # Define image parameterization using FFT for smooth results\n",
                "    param_f = lambda: param.image(image_size, fft=True, decorrelate=True)\n",
                "\n",
                "    # Run activation maximization\n",
                "    images = render.render_vis(\n",
                "        model,\n",
                "        objective,\n",
                "        param_f=param_f,\n",
                "        thresholds=(num_steps,),\n",
                "        show_image=show,\n",
                "        verbose=False,\n",
                "    )\n",
                "\n",
                "    # images is a list of images at different thresholds\n",
                "    # We take the final result\n",
                "    result_image = images[0][0]  # Shape: (H, W, 3)\n",
                "\n",
                "    return result_image\n",
                "\n",
                "\n",
                "# Visualize neuron 0 of mixed4a\n",
                "print(\"=\"*60)\n",
                "print(\"SINGLE NEURON VISUALIZATION DEMO\")\n",
                "print(\"=\"*60)\n",
                "demo_image = visualize_single_neuron(\n",
                "    model,\n",
                "    layer_name='mixed4a',\n",
                "    neuron_idx=0,\n",
                "    image_size=128,\n",
                "    num_steps=512,\n",
                "    show=True\n",
                ")\n",
                "\n",
                "# Plot the generated visualization with matplotlib\n",
                "plt.figure(figsize=(8, 8))\n",
                "plt.imshow(np.clip(demo_image, 0, 1))\n",
                "plt.title(\n",
                "    'Activation Maximization: mixed4a:0\\n'\n",
                "    'Image that maximally activates neuron 0',\n",
                "    fontsize=12,\n",
                "    fontweight='bold'\n",
                ")\n",
                "plt.axis('off')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Understanding the Visualization\n",
                "\n",
                "The generated image shows patterns that **maximally activate** this neuron.\n",
                "Key observations:\n",
                "\n",
                "1. **Repeating patterns**: The same motif appears throughout the image\n",
                "   because this maximizes the spatial average of the neuron's activation\n",
                "\n",
                "2. **Specific colors/orientations**: The neuron may prefer certain\n",
                "   color combinations or edge orientations\n",
                "\n",
                "3. **Abstract but structured**: Unlike random noise, these patterns\n",
                "   have clear structure that reveals what the neuron \"looks for\"\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Visualizing First 10 Neurons of mixed4a\n",
                "\n",
                "Now we'll generate activation maximization images for neurons 0-9 of the\n",
                "`mixed4a` layer and present them in a grid visualization, similar to the\n",
                "Distill.pub formatting."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\"\"\"Generate activation maximization visualizations for multiple neurons.\n",
                "\n",
                "This cell creates visualizations for the first 10 neurons of mixed4a\n",
                "and stores them for later display.\n",
                "\"\"\"\n",
                "\n",
                "\n",
                "def generate_neuron_visualizations(\n",
                "    model: torch.nn.Module,\n",
                "    layer_name: str,\n",
                "    neuron_indices: list,\n",
                "    image_size: int = 128,\n",
                "    num_steps: int = 512\n",
                ") -> dict:\n",
                "    \"\"\"Generate activation maximization images for multiple neurons.\n",
                "\n",
                "    Args:\n",
                "        model: Pre-trained neural network.\n",
                "        layer_name: Target layer name.\n",
                "        neuron_indices: List of neuron indices to visualize.\n",
                "        image_size: Output image resolution.\n",
                "        num_steps: Optimization iterations per neuron.\n",
                "\n",
                "    Returns:\n",
                "        dict: Mapping from neuron index to visualization image.\n",
                "\n",
                "    Example:\n",
                "        >>> images = generate_neuron_visualizations(\n",
                "        ...     model, 'mixed4a', range(10)\n",
                "        ... )\n",
                "        >>> images[0].shape  # (128, 128, 3)\n",
                "    \"\"\"\n",
                "    visualizations = {}\n",
                "\n",
                "    # Define image parameterization once (reused for each neuron)\n",
                "    param_f = lambda: param.image(image_size, fft=True, decorrelate=True)\n",
                "\n",
                "    total = len(neuron_indices)\n",
                "    for i, neuron_idx in enumerate(neuron_indices):\n",
                "        objective = f\"{layer_name}:{neuron_idx}\"\n",
                "        print(f\"[{i+1}/{total}] Generating {objective}...\", end=\" \")\n",
                "\n",
                "        # Generate visualization (suppress display)\n",
                "        images = render.render_vis(\n",
                "            model,\n",
                "            objective,\n",
                "            param_f=param_f,\n",
                "            thresholds=(num_steps,),\n",
                "            show_image=False,\n",
                "            verbose=False,\n",
                "        )\n",
                "\n",
                "        visualizations[neuron_idx] = images[0][0]\n",
                "        print(\"Done!\")\n",
                "\n",
                "    return visualizations\n",
                "\n",
                "\n",
                "# Generate visualizations for first 10 neurons\n",
                "print(\"=\"*60)\n",
                "print(\"GENERATING VISUALIZATIONS FOR FIRST 10 NEURONS OF mixed4a\")\n",
                "print(\"=\"*60)\n",
                "print(f\"Layer: mixed4a\")\n",
                "print(f\"Neurons: 0-9 (10 total)\")\n",
                "print(f\"Image size: 128x128\")\n",
                "print(f\"Optimization steps: 512\")\n",
                "print(\"-\"*60)\n",
                "\n",
                "neuron_visualizations = generate_neuron_visualizations(\n",
                "    model=model,\n",
                "    layer_name='mixed4a',\n",
                "    neuron_indices=list(range(10)),\n",
                "    image_size=128,\n",
                "    num_steps=512\n",
                ")\n",
                "\n",
                "print(\"-\"*60)\n",
                "print(f\"Generated {len(neuron_visualizations)} visualizations!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Grid Visualization\n",
                "\n",
                "Display all 10 neuron visualizations in a clean 2×5 grid layout,\n",
                "following the Distill.pub visual style."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\"\"\"Display neuron visualizations in a publication-quality grid layout.\n",
                "\n",
                "Creates a 2x5 grid of activation maximization images with labels,\n",
                "similar to the Distill.pub Circuits visualization style.\n",
                "\"\"\"\n",
                "\n",
                "\n",
                "def display_neuron_grid(\n",
                "    visualizations: dict,\n",
                "    layer_name: str,\n",
                "    ncols: int = 5,\n",
                "    figsize: tuple = (15, 7),\n",
                "    title: Optional[str] = None\n",
                ") -> plt.Figure:\n",
                "    \"\"\"Display neuron visualizations in a grid layout.\n",
                "\n",
                "    Args:\n",
                "        visualizations: Dict mapping neuron indices to images.\n",
                "        layer_name: Name of the layer for labeling.\n",
                "        ncols: Number of columns in the grid.\n",
                "        figsize: Figure size (width, height).\n",
                "        title: Optional title for the entire figure.\n",
                "\n",
                "    Returns:\n",
                "        plt.Figure: The matplotlib figure object.\n",
                "    \"\"\"\n",
                "    # Calculate grid dimensions\n",
                "    n_images = len(visualizations)\n",
                "    nrows = (n_images + ncols - 1) // ncols\n",
                "\n",
                "    # Create figure with constrained layout\n",
                "    fig, axes = plt.subplots(\n",
                "        nrows, ncols,\n",
                "        figsize=figsize,\n",
                "        constrained_layout=True\n",
                "    )\n",
                "\n",
                "    # Flatten axes for easy iteration\n",
                "    if nrows == 1:\n",
                "        axes = axes.reshape(1, -1)\n",
                "    axes_flat = axes.flatten()\n",
                "\n",
                "    # Plot each visualization\n",
                "    sorted_indices = sorted(visualizations.keys())\n",
                "    for ax_idx, neuron_idx in enumerate(sorted_indices):\n",
                "        ax = axes_flat[ax_idx]\n",
                "        image = visualizations[neuron_idx]\n",
                "\n",
                "        # Ensure image is in valid range [0, 1] for display\n",
                "        image_clipped = np.clip(image, 0, 1)\n",
                "\n",
                "        ax.imshow(image_clipped)\n",
                "        ax.set_title(f\"{layer_name}:{neuron_idx}\", fontsize=11, fontweight='bold')\n",
                "        ax.axis('off')\n",
                "\n",
                "    # Hide unused subplots\n",
                "    for ax_idx in range(len(sorted_indices), len(axes_flat)):\n",
                "        axes_flat[ax_idx].axis('off')\n",
                "        axes_flat[ax_idx].set_visible(False)\n",
                "\n",
                "    # Add main title if provided\n",
                "    if title:\n",
                "        fig.suptitle(title, fontsize=14, fontweight='bold', y=1.02)\n",
                "\n",
                "    return fig\n",
                "\n",
                "\n",
                "# Display the grid\n",
                "fig = display_neuron_grid(\n",
                "    neuron_visualizations,\n",
                "    layer_name='mixed4a',\n",
                "    ncols=5,\n",
                "    figsize=(16, 7),\n",
                "    title='Activation Maximization: First 10 Neurons of InceptionV1 mixed4a'\n",
                ")\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 5. Analysis and Interpretation\n",
                "\n",
                "### What Do These Neurons Detect?\n",
                "\n",
                "Looking at the generated visualizations, we can interpret what each\n",
                "neuron has learned to detect. Common feature types in `mixed4a` include:\n",
                "\n",
                "| Feature Type | Characteristics | Example Appearance |\n",
                "|-------------|-----------------|--------------------|\n",
                "| **Edges** | Oriented lines, gradients | Parallel stripes |\n",
                "| **Textures** | Repeating patterns | Grid-like or wave patterns |\n",
                "| **Colors** | Color combinations | Specific hue gradients |\n",
                "| **Curves** | Circular arcs, spirals | Curved line segments |\n",
                "| **Corners** | Angle detection | L-shaped or T-junction patterns |\n",
                "\n",
                "As noted in the Distill.pub [Early Vision](https://distill.pub/2020/circuits/early-vision/) article:\n",
                "\n",
                "> *\"Early layers detect simple features like edges and colors.\n",
                "> As we go deeper, features become more complex and semantic.\"*"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\"\"\"Provide detailed view of individual neurons with interpretation.\n",
                "\n",
                "This cell displays each neuron with more space for detailed examination.\n",
                "\"\"\"\n",
                "\n",
                "\n",
                "def display_detailed_neurons(\n",
                "    visualizations: dict,\n",
                "    layer_name: str,\n",
                "    neurons_per_row: int = 2\n",
                ") -> plt.Figure:\n",
                "    \"\"\"Display neurons with larger images for detailed analysis.\n",
                "\n",
                "    Args:\n",
                "        visualizations: Dict mapping neuron indices to images.\n",
                "        layer_name: Name of the layer.\n",
                "        neurons_per_row: Number of neurons to show per row.\n",
                "\n",
                "    Returns:\n",
                "        plt.Figure: The matplotlib figure object.\n",
                "    \"\"\"\n",
                "    n_neurons = len(visualizations)\n",
                "    n_rows = (n_neurons + neurons_per_row - 1) // neurons_per_row\n",
                "\n",
                "    fig, axes = plt.subplots(\n",
                "        n_rows, neurons_per_row,\n",
                "        figsize=(12, 5 * n_rows),\n",
                "        constrained_layout=True\n",
                "    )\n",
                "\n",
                "    axes_flat = axes.flatten() if n_rows > 1 else [axes] if neurons_per_row == 1 else axes\n",
                "\n",
                "    sorted_indices = sorted(visualizations.keys())\n",
                "    for ax_idx, neuron_idx in enumerate(sorted_indices):\n",
                "        if ax_idx >= len(axes_flat):\n",
                "            break\n",
                "\n",
                "        ax = axes_flat[ax_idx]\n",
                "        image = np.clip(visualizations[neuron_idx], 0, 1)\n",
                "\n",
                "        ax.imshow(image, interpolation='lanczos')\n",
                "        ax.set_title(\n",
                "            f\"{layer_name}:{neuron_idx}\",\n",
                "            fontsize=14,\n",
                "            fontweight='bold',\n",
                "            pad=10\n",
                "        )\n",
                "        ax.axis('off')\n",
                "\n",
                "    # Hide unused axes\n",
                "    for ax_idx in range(len(sorted_indices), len(axes_flat)):\n",
                "        if ax_idx < len(axes_flat):\n",
                "            axes_flat[ax_idx].set_visible(False)\n",
                "\n",
                "    fig.suptitle(\n",
                "        'Detailed View: mixed4a Neurons',\n",
                "        fontsize=16,\n",
                "        fontweight='bold',\n",
                "        y=1.01\n",
                "    )\n",
                "\n",
                "    return fig\n",
                "\n",
                "\n",
                "# Show first 4 neurons in detail\n",
                "subset_viz = {k: neuron_visualizations[k] for k in list(neuron_visualizations.keys())[:4]}\n",
                "fig = display_detailed_neurons(subset_viz, 'mixed4a', neurons_per_row=2)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 6. Advanced: Custom Optimization Parameters\n",
                "\n",
                "We can customize the optimization process to generate different styles\n",
                "of visualizations. Key parameters include:\n",
                "\n",
                "### Image Parameterization\n",
                "\n",
                "- **FFT (Fourier) parameterization**: Produces smoother, more natural-looking images\n",
                "- **Pixel parameterization**: Direct optimization in pixel space (often noisier)\n",
                "- **Decorrelation**: Optimizes in a decorrelated color space\n",
                "\n",
                "### Transformations\n",
                "\n",
                "Random transformations during optimization improve **robustness**:\n",
                "- **Jitter**: Small random translations\n",
                "- **Scale**: Random zoom in/out\n",
                "- **Rotate**: Small random rotations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\"\"\"Demonstrate different parameterization options.\n",
                "\n",
                "Compare FFT-based vs pixel-based parameterization to show\n",
                "the importance of proper image parameterization.\n",
                "\"\"\"\n",
                "\n",
                "\n",
                "def compare_parameterizations(\n",
                "    model: torch.nn.Module,\n",
                "    layer_name: str,\n",
                "    neuron_idx: int,\n",
                "    image_size: int = 128,\n",
                "    num_steps: int = 256\n",
                ") -> tuple:\n",
                "    \"\"\"Compare FFT vs pixel parameterization for the same neuron.\n",
                "\n",
                "    Args:\n",
                "        model: The neural network model.\n",
                "        layer_name: Target layer name.\n",
                "        neuron_idx: Neuron index to visualize.\n",
                "        image_size: Output image resolution.\n",
                "        num_steps: Optimization iterations.\n",
                "\n",
                "    Returns:\n",
                "        tuple: (fft_image, pixel_image) as numpy arrays.\n",
                "    \"\"\"\n",
                "    objective = f\"{layer_name}:{neuron_idx}\"\n",
                "\n",
                "    # FFT parameterization (smooth)\n",
                "    print(f\"Generating with FFT parameterization...\")\n",
                "    param_fft = lambda: param.image(image_size, fft=True, decorrelate=True)\n",
                "    images_fft = render.render_vis(\n",
                "        model, objective,\n",
                "        param_f=param_fft,\n",
                "        thresholds=(num_steps,),\n",
                "        show_image=False,\n",
                "        verbose=False\n",
                "    )\n",
                "\n",
                "    # Pixel parameterization (often noisier)\n",
                "    print(f\"Generating with pixel parameterization...\")\n",
                "    param_pixel = lambda: param.image(image_size, fft=False, decorrelate=False)\n",
                "    images_pixel = render.render_vis(\n",
                "        model, objective,\n",
                "        param_f=param_pixel,\n",
                "        thresholds=(num_steps,),\n",
                "        show_image=False,\n",
                "        verbose=False\n",
                "    )\n",
                "\n",
                "    return images_fft[0][0], images_pixel[0][0]\n",
                "\n",
                "\n",
                "# Compare for neuron 5\n",
                "print(\"Comparing parameterization methods for mixed4a:5\")\n",
                "print(\"-\" * 50)\n",
                "fft_img, pixel_img = compare_parameterizations(\n",
                "    model, 'mixed4a', 5, image_size=128, num_steps=256\n",
                ")\n",
                "\n",
                "# Display comparison\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
                "\n",
                "axes[0].imshow(np.clip(fft_img, 0, 1))\n",
                "axes[0].set_title('FFT Parameterization\\n(Smoother, more natural)', fontsize=12)\n",
                "axes[0].axis('off')\n",
                "\n",
                "axes[1].imshow(np.clip(pixel_img, 0, 1))\n",
                "axes[1].set_title('Pixel Parameterization\\n(More high-frequency noise)', fontsize=12)\n",
                "axes[1].axis('off')\n",
                "\n",
                "fig.suptitle('Effect of Image Parameterization on Feature Visualization',\n",
                "             fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 7. High-Resolution Visualization\n",
                "\n",
                "For publication-quality images, we can generate higher resolution\n",
                "visualizations with more optimization steps."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\"\"\"Generate high-resolution visualization for a selected neuron.\n",
                "\n",
                "Higher resolution and more optimization steps produce cleaner,\n",
                "more detailed feature visualizations.\n",
                "\"\"\"\n",
                "\n",
                "\n",
                "def generate_high_res_visualization(\n",
                "    model: torch.nn.Module,\n",
                "    layer_name: str,\n",
                "    neuron_idx: int,\n",
                "    image_size: int = 256,\n",
                "    num_steps: int = 1024\n",
                ") -> np.ndarray:\n",
                "    \"\"\"Generate high-resolution activation maximization image.\n",
                "\n",
                "    Args:\n",
                "        model: The neural network model.\n",
                "        layer_name: Target layer name.\n",
                "        neuron_idx: Neuron index to visualize.\n",
                "        image_size: Output resolution (larger = more detail).\n",
                "        num_steps: Optimization iterations (more = cleaner).\n",
                "\n",
                "    Returns:\n",
                "        np.ndarray: High-resolution visualization image.\n",
                "\n",
                "    Notes:\n",
                "        Higher resolution requires more VRAM. Reduce image_size\n",
                "        if you encounter out-of-memory errors.\n",
                "    \"\"\"\n",
                "    objective = f\"{layer_name}:{neuron_idx}\"\n",
                "    print(f\"Generating high-res visualization for {objective}\")\n",
                "    print(f\"  Resolution: {image_size}x{image_size}\")\n",
                "    print(f\"  Optimization steps: {num_steps}\")\n",
                "    print(\"  This may take a minute...\")\n",
                "\n",
                "    param_f = lambda: param.image(image_size, fft=True, decorrelate=True)\n",
                "\n",
                "    images = render.render_vis(\n",
                "        model,\n",
                "        objective,\n",
                "        param_f=param_f,\n",
                "        thresholds=(num_steps,),\n",
                "        show_image=False,\n",
                "        verbose=False\n",
                "    )\n",
                "\n",
                "    return images[0][0]\n",
                "\n",
                "\n",
                "# Generate high-res visualization for neuron 3\n",
                "high_res_image = generate_high_res_visualization(\n",
                "    model,\n",
                "    layer_name='mixed4a',\n",
                "    neuron_idx=3,\n",
                "    image_size=256,\n",
                "    num_steps=1024\n",
                ")\n",
                "\n",
                "# Display\n",
                "plt.figure(figsize=(10, 10))\n",
                "plt.imshow(np.clip(high_res_image, 0, 1))\n",
                "plt.title('High-Resolution Visualization: mixed4a:3\\n(256×256, 1024 steps)',\n",
                "          fontsize=14, fontweight='bold')\n",
                "plt.axis('off')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## 8. Summary and Key Takeaways\n",
                "\n",
                "### What We Learned\n",
                "\n",
                "1. **Activation Maximization** reveals what patterns maximally activate specific neurons\n",
                "\n",
                "2. **InceptionV1's mixed4a layer** contains diverse features:\n",
                "   - Edges and gradients\n",
                "   - Textures and patterns\n",
                "   - Color preferences\n",
                "   - Early shape detectors\n",
                "\n",
                "3. **Lucent/Lucid** makes feature visualization accessible with:\n",
                "   - Pre-trained models with correct layer naming\n",
                "   - FFT-based image parameterization for smooth results\n",
                "   - Transformation robustness for reliable visualizations\n",
                "\n",
                "### Connection to Circuits Research\n",
                "\n",
                "The Distill.pub [Circuits](https://distill.pub/2020/circuits/) project shows that:\n",
                "\n",
                "> Neural networks contain **interpretable features** (neurons) that combine to form\n",
                "> **circuits** (connected groups of neurons) which implement meaningful algorithms.\n",
                "\n",
                "This notebook demonstrated the first step: **visualizing individual features**.\n",
                "Future work could explore:\n",
                "- How features combine into circuits\n",
                "- Feature evolution across layers\n",
                "- Object-specific vs texture-specific neurons\n",
                "\n",
                "### References\n",
                "\n",
                "1. Olah, C., et al. (2020). [Zoom In: An Introduction to Circuits](https://distill.pub/2020/circuits/zoom-in/). *Distill*.\n",
                "2. Olah, C., et al. (2020). [An Overview of Early Vision in InceptionV1](https://distill.pub/2020/circuits/early-vision/). *Distill*.\n",
                "3. Olah, C., et al. (2017). [Feature Visualization](https://distill.pub/2017/feature-visualization/). *Distill*.\n",
                "4. Mordvintsev, A., et al. (2018). [Differentiable Image Parameterizations](https://distill.pub/2018/differentiable-parameterizations/). *Distill*."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\"\"\"Final summary: Display all 10 neurons with enhanced formatting.\n",
                "\n",
                "This provides a clean, publication-ready summary visualization.\n",
                "\"\"\"\n",
                "\n",
                "# Create final summary figure\n",
                "fig = plt.figure(figsize=(18, 8))\n",
                "\n",
                "# Title and description\n",
                "fig.suptitle(\n",
                "    'Activation Maximization: First 10 Neurons of InceptionV1 mixed4a Layer',\n",
                "    fontsize=16,\n",
                "    fontweight='bold',\n",
                "    y=0.98\n",
                ")\n",
                "\n",
                "# Create grid\n",
                "gs = GridSpec(2, 5, figure=fig, hspace=0.3, wspace=0.1)\n",
                "\n",
                "for idx in range(10):\n",
                "    row = idx // 5\n",
                "    col = idx % 5\n",
                "\n",
                "    ax = fig.add_subplot(gs[row, col])\n",
                "    image = np.clip(neuron_visualizations[idx], 0, 1)\n",
                "\n",
                "    ax.imshow(image)\n",
                "    ax.set_title(f'Neuron {idx}', fontsize=11, fontweight='bold')\n",
                "    ax.axis('off')\n",
                "\n",
                "# Add methodology note\n",
                "fig.text(\n",
                "    0.5, 0.02,\n",
                "    'Generated using Lucent library with FFT parameterization, 512 optimization steps',\n",
                "    ha='center',\n",
                "    fontsize=10,\n",
                "    style='italic',\n",
                "    alpha=0.7\n",
                ")\n",
                "\n",
                "plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
                "plt.show()\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"NOTEBOOK COMPLETE\")\n",
                "print(\"=\"*60)\n",
                "print(\"Successfully reproduced activation maximization visualizations\")\n",
                "print(\"for the first 10 neurons of InceptionV1's mixed4a layer.\")\n",
                "print(\"\\nKey libraries used:\")\n",
                "print(\"  - torch-lucent (PyTorch port of Lucid)\")\n",
                "print(\"  - matplotlib (visualization)\")\n",
                "print(\"  - numpy (array operations)\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
